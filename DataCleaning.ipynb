{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, TweetTokenizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_train = pd.read_csv(r\"C:\\Users\\samsu\\Desktop\\Fall20\\CS 695-002\\Project\\Datasets\\Final_Data\\English.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \n",
    "    #removes URL\n",
    "    pattern = r'https.?://[^\\s]+[\\s]?'\n",
    "    df[\"processed\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "    \n",
    "    #removes usernames/mentions\n",
    "    pattern = r'@[^\\s]+'\n",
    "    df[\"processed\"] = df[\"processed\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "    \n",
    "    #removes emoji and smiley\n",
    "    pattern = re.compile(\"[\"\n",
    "                         u\"\\U0001F600-\\U0001F64F\"\n",
    "                         u\"\\U0001F300-\\U0001F5FF\"\n",
    "                         u\"\\U0001F680-\\U0001F6FF\"\n",
    "                         u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                         u\"\\U00002500-\\U00002BEF\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U00002702-\\U000027B0\"\n",
    "                         u\"\\U000024C2-\\U0001F251\"\n",
    "                         u\"\\U0001f926-\\U0001f937\"\n",
    "                         u\"\\U00010000-\\U0010ffff\"\n",
    "                         u\"\\u2640-\\u2642\"\n",
    "                         u\"\\u2600-\\u2B55\"\n",
    "                         u\"\\u200d\"\n",
    "                         u\"\\u23cf\"\n",
    "                         u\"\\u23e9\"\n",
    "                         u\"\\u231a\"\n",
    "                         u\"\\ufe0f\"\n",
    "                         u\"\\u3030\"\n",
    "                         \"]+\", flags=re.UNICODE)\n",
    "    df[\"processed\"] = df[\"processed\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "    \n",
    "    #removes numbers\n",
    "    pattern = r'\\d+'\n",
    "    df[\"processed\"] = df[\"processed\"].str.replace(pat=pattern, repl=\"\", regex=True)\n",
    "    \n",
    "    #removes punctuation\n",
    "    pattern = r\"[^\\w\\s]\"\n",
    "    df[\"processed\"] = df[\"processed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "    \n",
    "    #converts to lower case\n",
    "    #df[\"processed\"] = df[\"processed\"].str.lower()\n",
    "\n",
    "    #removes stop words\n",
    "    stop_words = stopwords.words(\"english\")    \n",
    "    remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \")\n",
    "                                              if token not in stop_words])\n",
    "    df[\"processed\"] = df[\"processed\"].apply(remove_stop_words)\n",
    "    \n",
    "    #removes extra spaces\n",
    "    pattern = r\"[\\s]+\"\n",
    "    df[\"processed\"] = df[\"processed\"].str.replace(pat=pattern, repl=\" \", regex=True)\n",
    "    \n",
    "    #extract root words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    w_tokenizer = TweetTokenizer()\n",
    "    def lemmatize_text(text):\n",
    "         return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "    \n",
    "    df[\"processed\"] = df[\"processed\"].apply(lambda row: lemmatize_text(row))\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.77 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As of March 13th , 2014 , the booklet had been...</td>\n",
       "      <td>0</td>\n",
       "      <td>[As, March, th, booklet, downloaded, time, cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to help increase the booklets downloa...</td>\n",
       "      <td>0</td>\n",
       "      <td>[In, order, help, increase, booklet, downloads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>( Simply copy and paste the following text int...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Simply, copy, paste, following, text, YouTube...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Click below for a FREE download of a colorfull...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Click, FREE, download, colorfully, illustrate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Click, DOWNLOAD, MB, green, banner, link]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67326</th>\n",
       "      <td>RT @JakeM_1998: RT BillSpindle: It's all about...</td>\n",
       "      <td>0</td>\n",
       "      <td>[RT, RT, BillSpindle, It, power, top, fighter,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67327</th>\n",
       "      <td>RT @ThinkAgain_DOS: Iraq: #ISIS sets off 21 ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>[RT, Iraq, ISIS, set, car, bomb, Anbar, usual,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67328</th>\n",
       "      <td>RT @ThePatriot143: DEAR STATE DEPARTMENT: WHER...</td>\n",
       "      <td>0</td>\n",
       "      <td>[RT, DEAR, STATE, DEPARTMENT, WHERE, IS, HILLA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67329</th>\n",
       "      <td>\"@panelrific: Let's go üêßüêßüêßüêßüêßüêßüòÉ\"</td>\n",
       "      <td>0</td>\n",
       "      <td>[Let, go]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67330</th>\n",
       "      <td>RT @TheMeninism: üòÇ http://t.co/VQzuAXqNzd</td>\n",
       "      <td>0</td>\n",
       "      <td>[RT, http, co, VQzuAXqNzd]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67331 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  label  \\\n",
       "0      As of March 13th , 2014 , the booklet had been...      0   \n",
       "1      In order to help increase the booklets downloa...      0   \n",
       "2      ( Simply copy and paste the following text int...      0   \n",
       "3      Click below for a FREE download of a colorfull...      1   \n",
       "4      Click on the `` DOWNLOAD ( 7.42 MB ) '' green ...      0   \n",
       "...                                                  ...    ...   \n",
       "67326  RT @JakeM_1998: RT BillSpindle: It's all about...      0   \n",
       "67327  RT @ThinkAgain_DOS: Iraq: #ISIS sets off 21 ca...      0   \n",
       "67328  RT @ThePatriot143: DEAR STATE DEPARTMENT: WHER...      0   \n",
       "67329                    \"@panelrific: Let's go üêßüêßüêßüêßüêßüêßüòÉ\"      0   \n",
       "67330          RT @TheMeninism: üòÇ http://t.co/VQzuAXqNzd      0   \n",
       "\n",
       "                                               processed  \n",
       "0      [As, March, th, booklet, downloaded, time, cou...  \n",
       "1      [In, order, help, increase, booklet, downloads...  \n",
       "2      [Simply, copy, paste, following, text, YouTube...  \n",
       "3      [Click, FREE, download, colorfully, illustrate...  \n",
       "4             [Click, DOWNLOAD, MB, green, banner, link]  \n",
       "...                                                  ...  \n",
       "67326  [RT, RT, BillSpindle, It, power, top, fighter,...  \n",
       "67327  [RT, Iraq, ISIS, set, car, bomb, Anbar, usual,...  \n",
       "67328  [RT, DEAR, STATE, DEPARTMENT, WHERE, IS, HILLA...  \n",
       "67329                                          [Let, go]  \n",
       "67330                         [RT, http, co, VQzuAXqNzd]  \n",
       "\n",
       "[67331 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "preprocess(en_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are more concerned about ZOG controlling US and Russia .\n",
      "['We', 'concerned', 'ZOG', 'controlling', 'US', 'Russia']\n"
     ]
    }
   ],
   "source": [
    "print(en_train.tweet[199])\n",
    "print(en_train.processed[199])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#en_train[en_train['tweet'].astype(str).str.contains('US')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources = [\"wordnet\", \"stopwords\", \"punkt\", \"averaged_perceptron_tagger\", \"maxent_treebank_pos_tagger\"]\n",
    "\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            nltk.data.find(\"tokenizers/\" + resource)\n",
    "        except LookupError:\n",
    "            nltk.download(resource)\n",
    "    \n",
    "    #create Lemmatizer object\n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize_word(tagged_token):\n",
    "        root = []\n",
    "        for token in tagged_token:\n",
    "            tag = token[1][0]\n",
    "            word = token[0]\n",
    "            if tag.startswith('J'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.ADJ))\n",
    "            elif tag.startswith('V'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.VERB))\n",
    "            elif tag.startswith('N'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.NOUN))\n",
    "            elif tag.startswith('R'):\n",
    "                root.append(lemma.lemmatize(word, wordnet.ADV))\n",
    "            else:          \n",
    "                root.append(word)\n",
    "        return root\n",
    "    \n",
    "    def lemmatize_doc(document):\n",
    "        lemmatized_list = []\n",
    "        tokenized_sent = sent_tokenize(document)\n",
    "        for sentence in tokenized_sent:\n",
    "            no_punctuation = re.sub(r\"[`'\\\",.!?()]\", \" \", sentence)\n",
    "            tokenized_word = word_tokenize(no_punctuation)\n",
    "            tagged_token = pos_tag(tokenized_word)\n",
    "            lemmatized = lemmatize_word(tagged_token)\n",
    "            lemmatized_list.extend(lemmatized)\n",
    "        return \" \".join(lemmatized_list)\n",
    "    \n",
    "    #apply the functions\n",
    "    df[\"processed\"] = df[\"processed\"].apply(lambda row: lemmatize_doc(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
