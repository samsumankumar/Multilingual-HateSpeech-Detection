{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":" BERT_english.ipynb","provenance":[{"file_id":"1UJPUgsgtZnh66_9KPHyaxdlL6NSfhWZQ","timestamp":1607211502764}],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyPRaDCPJyqHfmlmGrAmZkxM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6ae013abbede4a3da0d1975aed1b7eb2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0f470eb7ad3d4dc8b2da94dfca7792d5","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_958912d90b8349a8a2ea34cd1cb3400e","IPY_MODEL_9a065fbf09e04f9b8fccc928a2a889c5"]}},"0f470eb7ad3d4dc8b2da94dfca7792d5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"958912d90b8349a8a2ea34cd1cb3400e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f86ab8821a36432d8864aea74f93be14","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_95ea5512255643379917aa04f8e41cd4"}},"9a065fbf09e04f9b8fccc928a2a889c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d844ac55d9304c66b0ab04c04ab102f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:06&lt;00:00, 66.9B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_67ffd389be2c49b092625654bf737339"}},"f86ab8821a36432d8864aea74f93be14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"95ea5512255643379917aa04f8e41cd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d844ac55d9304c66b0ab04c04ab102f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"67ffd389be2c49b092625654bf737339":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e01a652b6eb64ba5a400358c659e200f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3204c8f41d9c4e68b896bd98ada009fd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3d877883631948bd8e8e47dd0b2e43c2","IPY_MODEL_10dc7f638118467cb8c5c5e9026ef06b"]}},"3204c8f41d9c4e68b896bd98ada009fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3d877883631948bd8e8e47dd0b2e43c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6c74e9439e8d4fca9890b1c7338523a3","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0384abb9be5b4334a282f0603045d3c8"}},"10dc7f638118467cb8c5c5e9026ef06b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a84afdc1aed44029bab9eb0fda0e60aa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:06&lt;00:00, 72.2MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4a905ca78d7242c2b817134e72e2558f"}},"6c74e9439e8d4fca9890b1c7338523a3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0384abb9be5b4334a282f0603045d3c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a84afdc1aed44029bab9eb0fda0e60aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4a905ca78d7242c2b817134e72e2558f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc0d42b069654e3aa807fde45e27e751":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f0760d74589641c599a3b3603a72b74d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_aad182a66a5a44659a3f6bd7c276d0b3","IPY_MODEL_d3a169552fac409aa01a8b94e0395cd2"]}},"f0760d74589641c599a3b3603a72b74d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"aad182a66a5a44659a3f6bd7c276d0b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e999248575bd43138a3b68aa0ed55651","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b5c453e9073c4ce49989eec55ab1148e"}},"d3a169552fac409aa01a8b94e0395cd2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_27c9fcb6da1a4989ab431dfd1673af86","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 636kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a56caae4d020400fada6ae6c706a5c0c"}},"e999248575bd43138a3b68aa0ed55651":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b5c453e9073c4ce49989eec55ab1148e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27c9fcb6da1a4989ab431dfd1673af86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a56caae4d020400fada6ae6c706a5c0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fvy2c887aK4y","executionInfo":{"status":"ok","timestamp":1607239956644,"user_tz":300,"elapsed":21562,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"deba21af-d744-43cd-fd4a-6fbc7069df3b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nU58fyX3aWYT","executionInfo":{"status":"ok","timestamp":1607239963434,"user_tz":300,"elapsed":28335,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"f75bcefe-15b6-4342-ce37-1654fc3b42d6"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/84/7bc03215279f603125d844bf81c3fb3f2d50fe8e511546eb4897e4be2067/transformers-4.0.0-py3-none-any.whl (1.4MB)\n","\r\u001b[K     |▎                               | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 29.6MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 25.7MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 19.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 16.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 18.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 14.4MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 15.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 15.9MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112kB 15.0MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 133kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 143kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▋                            | 153kB 15.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 174kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▍                           | 184kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▋                           | 194kB 15.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 204kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 215kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 225kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 235kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 245kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 256kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 266kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 276kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 286kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 296kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 307kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 317kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 327kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 337kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 348kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 358kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 368kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 378kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 389kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 399kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 409kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 419kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 430kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 440kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 450kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 460kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 471kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 481kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 491kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 501kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 512kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 522kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 532kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 542kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 552kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 563kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 573kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 583kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 593kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 604kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 614kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 624kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 634kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 645kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 655kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 665kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 675kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 686kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 696kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 706kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 716kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 727kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 737kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 747kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 757kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 768kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 778kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 788kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 798kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 808kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 819kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 829kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 839kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 849kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 860kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 870kB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 880kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 890kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 901kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 911kB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 921kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 931kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 942kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 952kB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 962kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 972kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 983kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 993kB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.2MB 15.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 15.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 15.0MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 53.0MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 68.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=c8ab949093fd695f727005e313f1752e004b9cb05253c748125eb3da5859b431\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGpvl8-iBLSi","executionInfo":{"status":"ok","timestamp":1607239967341,"user_tz":300,"elapsed":32235,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"728c1d3a-4794-459c-ae35-31b85d653398"},"source":["!pip install emoji"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n","\r\u001b[K     |██████▍                         | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 26.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.4MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=2f8f094ca033ee68ca390e0bd151eb6180cf4ed1309c96a57326b5e04099bf72\n","  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"4KyBcSEbadZQ","executionInfo":{"status":"ok","timestamp":1607239972651,"user_tz":300,"elapsed":37536,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"f30bd82a-ac0b-45e9-f776-069c3f159526"},"source":["import tensorflow as tf\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertForSequenceClassification, BertTokenizer\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import os\n","import re\n","import emoji\n","import random\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, TweetTokenizer\n","from nltk.corpus import wordnet, stopwords\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, accuracy_score\n","from statistics import mode\n","\n","\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla V100-SXM2-16GB'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"2_52J_OQ2dA6","executionInfo":{"status":"ok","timestamp":1607239972652,"user_tz":300,"elapsed":37529,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def preprocess(df):\n","    \n","    #removes URL\n","    pattern = r'https.?://[^\\s]+[\\s]?'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes usernames/mentions\n","    pattern = r'@[^\\s]+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes emoji and smiley\n","    pattern = re.compile(\"[\"\n","                         u\"\\U0001F600-\\U0001F64F\"\n","                         u\"\\U0001F300-\\U0001F5FF\"\n","                         u\"\\U0001F680-\\U0001F6FF\"\n","                         u\"\\U0001F1E0-\\U0001F1FF\"\n","                         u\"\\U00002500-\\U00002BEF\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U000024C2-\\U0001F251\"\n","                         u\"\\U0001f926-\\U0001f937\"\n","                         u\"\\U00010000-\\U0010ffff\"\n","                         u\"\\u2640-\\u2642\"\n","                         u\"\\u2600-\\u2B55\"\n","                         u\"\\u200d\"\n","                         u\"\\u23cf\"\n","                         u\"\\u23e9\"\n","                         u\"\\u231a\"\n","                         u\"\\ufe0f\"\n","                         u\"\\u3030\"\n","                         \"]+\", flags=re.UNICODE)\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes numbers\n","    pattern = r'\\d+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes punctuation\n","    pattern = r\"[^\\w\\s]\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","\n","    #removes stop words\n","    stop_words = stopwords.words(\"english\")    \n","    remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \")\n","                                              if token not in stop_words])\n","    df[\"tweet\"] = df[\"tweet\"].apply(remove_stop_words)\n","    \n","    #removes extra spaces\n","    pattern = r\"[\\s]+\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","    \n","    return(df)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATUTCjFaaknr","executionInfo":{"status":"ok","timestamp":1607239972653,"user_tz":300,"elapsed":37523,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def train_validate_test_split(df,seed, train_percent=.8, validate_percent=.125):\n","  train, test = train_test_split(df, train_size=train_percent, stratify=df['label'], random_state=seed)\n","  train, validate = train_test_split(train, test_size=validate_percent, stratify=train['label'], random_state=seed)\n","  return train, validate, test\n","\n","def sample_data(df,sample,seed):\n","    X_train, _, y_train, _ = train_test_split( df['tweet'], df['label'], train_size=sample, random_state=seed, stratify=df['label'])\n","    return pd.concat([X_train,y_train], axis = 1 )\n","\n","def tokenize_data(df):\n","    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in df['tweet']]\n","    # Tokenize with multilingual BERT tokenizer\n","    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","    #tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    \n","    MAX_LEN = 128\n","\n","    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    # Create attention masks\n","    attention_masks = []\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return input_ids, attention_masks\n","\n","def Data_Loader(inputs_ids, attention_masks, df,batch_size=16): \n","    data = TensorDataset(torch.LongTensor(inputs_ids), torch.LongTensor(attention_masks), torch.LongTensor(df['label'].values))\n","    sampler = RandomSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n","    return dataloader"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHohHSKIaq7b","executionInfo":{"status":"ok","timestamp":1607239972654,"user_tz":300,"elapsed":37518,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def model_train(model, train_dataloader, validation_dataloader):\n","    # BERT training loop\n","    epochs = 3\n","    for _ in trange(epochs, desc=\"Epoch\"):  \n","        # Set our model to training mode\n","        model.train()\n","        # Tracking variables\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        # Train the data for one epoch\n","        for step, batch in enumerate(train_dataloader):\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Clear out the gradients (by default they accumulate)\n","            optimizer.zero_grad()\n","            # Forward pass\n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","            loss = outputs[\"loss\"]\n","            # Backward pass\n","            loss.backward()\n","            # Update parameters and take a step using the computed gradient\n","            optimizer.step()\n","            # Update tracking variables\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","\n","        ## VALIDATION\n","\n","        # Put model in evaluation mode\n","        model.eval()\n","        # Tracking variables \n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","            with torch.no_grad():\n","              # Forward pass, calculate logit predictions\n","                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n","            eval_accuracy += tmp_eval_accuracy\n","            nb_eval_steps += 1\n","        validation_accuracy = (eval_accuracy/nb_eval_steps)\n","        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","    return validation_accuracy"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fh9pHbFPax9h","executionInfo":{"status":"ok","timestamp":1607239972655,"user_tz":300,"elapsed":37510,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_test(model,prediction_dataloader):\n","    model.eval()\n","    # Tracking variables \n","    predictions , true_labels = [], []\n","# Predict\n","    for batch in prediction_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        predictions+=list(np.argmax(logits, axis=1).flatten())\n","        true_labels+=list(label_ids.flatten())\n","    test_f1_score = f1_score(true_labels, predictions, average= 'macro')\n","    print(\"Macro F1 Score:\",test_f1_score)\n","    test_f1_wscore = f1_score(true_labels, predictions, average= 'weighted')\n","    print(\"Weighted F1 Score:\",test_f1_wscore)\n","    test_accuracy_score = accuracy_score(true_labels, predictions)\n","    print(\"Accuracy score:\", test_accuracy_score, \"\\n\")\n","    print(\"=\"*100)\n","    return test_f1_score, test_f1_wscore"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"C84QhrtEa5xO","executionInfo":{"status":"ok","timestamp":1607239972656,"user_tz":300,"elapsed":37500,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def model_initialise():\n","  # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","  model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).cuda()\n","  optimizer = optim.AdamW(params = model.parameters(), lr=2e-5)\n","  return model, optimizer"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6ae013abbede4a3da0d1975aed1b7eb2","0f470eb7ad3d4dc8b2da94dfca7792d5","958912d90b8349a8a2ea34cd1cb3400e","9a065fbf09e04f9b8fccc928a2a889c5","f86ab8821a36432d8864aea74f93be14","95ea5512255643379917aa04f8e41cd4","d844ac55d9304c66b0ab04c04ab102f9","67ffd389be2c49b092625654bf737339","e01a652b6eb64ba5a400358c659e200f","3204c8f41d9c4e68b896bd98ada009fd","3d877883631948bd8e8e47dd0b2e43c2","10dc7f638118467cb8c5c5e9026ef06b","6c74e9439e8d4fca9890b1c7338523a3","0384abb9be5b4334a282f0603045d3c8","a84afdc1aed44029bab9eb0fda0e60aa","4a905ca78d7242c2b817134e72e2558f","bc0d42b069654e3aa807fde45e27e751","f0760d74589641c599a3b3603a72b74d","aad182a66a5a44659a3f6bd7c276d0b3","d3a169552fac409aa01a8b94e0395cd2","e999248575bd43138a3b68aa0ed55651","b5c453e9073c4ce49989eec55ab1148e","27c9fcb6da1a4989ab431dfd1673af86","a56caae4d020400fada6ae6c706a5c0c"]},"id":"pxfLlgbVCCBv","executionInfo":{"status":"ok","timestamp":1607253797164,"user_tz":300,"elapsed":13861995,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"e07c4e40-c2e2-4c79-86cd-5cfced8d4246"},"source":["print_stmts= []\n","languages = ['English']\n","directory = 'drive/My Drive/CS695'\n","for lang in languages:\n","  df = pd.read_csv(os.path.join(directory, lang+'.csv'))\n","  df = preprocess(df)\n","  sample_sizes = [16, 32, 64, 128, 256]\n","for sample in sample_sizes: \n","  seeds = [2018,2019, 2020, 2021, 2022]\n","  weighted = []\n","  macro = []\n","  for seed in seeds:\n","    np.random.seed(seed)\n","    train_df, validation_df, test_df = train_validate_test_split(df, seed)\n","    train_len = len(train_df)\n","    if sample==256 and seed==2022:\n","        sample_sizes.append(train_len)\n","    if sample == train_len and seed == 2022:\n","        sample_sizes.remove(train_len)\n","    model, optimizer = model_initialise()\n","    if(sample != train_len):  \n","      train_df_sample = sample_data(train_df,sample,seed)\n","      train_input_ids, train_attention_masks = tokenize_data(train_df_sample)\n","      train_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_df_sample)\n","    else:\n","      train_input_ids, train_attention_masks = tokenize_data(train_df)\n","      train_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_df)\n","\n","    validation_input_ids, validation_attention_masks = tokenize_data(validation_df)\n","    validation_dataloader = Data_Loader(validation_input_ids, validation_attention_masks, validation_df)\n","    print(\"\\nModel Summary:\")\n","    print('Language:', lang)\n","    print('Sample Size:', sample)\n","    print('Seed value:', seed)\n","    validation_accuracy = model_train(model, train_dataloader, validation_dataloader)\n","    test_input_ids, test_attention_masks = tokenize_data(test_df)\n","    test_dataloader = Data_Loader(test_input_ids, test_attention_masks, test_df)\n","    m, w = model_test(model, test_dataloader)\n","    weighted.append(w)\n","    macro.append(m)\n","  print(\"The Average  Weighted F1-Score of the Language \", lang, \"is:\",sum(weighted)/ len(weighted))\n","  print(\"The Average  Macro F1-Score of the Language \", lang, \"is:\",sum(macro)/ len(macro))\n","  print(\"=\"*200)\n","  print_stmts.append(\" For Sample Size \"+str(sample)+\" Average Weighted F1-Score \"+str(sum(weighted)/len(weighted))+\" and Average Macro F1-Score \"+str(sum(macro)/len(macro))+\" of \"+ str(lang))\n","for i in print_stmts:\n","  print(i,\"\\n\")\n","print(\"=\"*100+str(lang)+\"=\"*100)"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ae013abbede4a3da0d1975aed1b7eb2","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e01a652b6eb64ba5a400358c659e200f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc0d42b069654e3aa807fde45e27e751","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 16\n","Seed value: 2018\n","Train loss: 0.7875593900680542\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.445201062215478\n","Train loss: 0.6731244325637817\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7660976226605969\n","Train loss: 0.6439403295516968\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.799772382397572\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4482126969173601\n","Weighted F1 Score: 0.7187710759965317\n","Accuracy score: 0.8001517594612539 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 16\n","Seed value: 2019\n","Train loss: 0.5847092866897583\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8065123925139099\n","Train loss: 0.519483208656311\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8066831057157309\n","Train loss: 0.5039225220680237\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.15s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4468043334899457\n","Weighted F1 Score: 0.7208237528542989\n","Accuracy score: 0.8068860855543963 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 16\n","Seed value: 2020\n","Train loss: 0.8405503034591675\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.2877845220030349\n","Train loss: 0.753589928150177\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7458965604451189\n","Train loss: 0.6672976613044739\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8013783510369247\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4489114586382593\n","Weighted F1 Score: 0.7198492862144559\n","Accuracy score: 0.8022858768851371 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 16\n","Seed value: 2021\n","Train loss: 0.7035276889801025\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.786210166919575\n","Train loss: 0.6347970366477966\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8032751643904906\n","Train loss: 0.5618079900741577\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8067842690945878\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465616797900262\n","Weighted F1 Score: 0.720733523978667\n","Accuracy score: 0.8068860855543963 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 16\n","Seed value: 2022\n","Train loss: 0.92399001121521\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.35701188669701567\n","Train loss: 0.7024259567260742\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7229577642893272\n","Train loss: 0.5937894582748413\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7968259989883663\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.45190427525506366\n","Weighted F1 Score: 0.7191683793573067\n","Accuracy score: 0.797638243384236 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.7198692036802521\n","The Average  Macro F1-Score of the Language  English is: 0.44847888881813097\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 32\n","Seed value: 2018\n","Train loss: 0.5736183822154999\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.49169690907001495\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069613050075873\n","Train loss: 0.4356447160243988\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069676277187657\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 32\n","Seed value: 2019\n","Train loss: 0.5648916661739349\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.5168718099594116\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.4956187754869461\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069676277187657\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 32\n","Seed value: 2020\n","Train loss: 0.769576907157898\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7131638846737481\n","Train loss: 0.6135363578796387\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8049001011633788\n","Train loss: 0.5170315206050873\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8068854324734446\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.446576205348941\n","Weighted F1 Score: 0.7207569676769022\n","Accuracy score: 0.8069335103860381 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 32\n","Seed value: 2021\n","Train loss: 0.8208950459957123\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.26s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.689339908952959\n","Train loss: 0.6238411664962769\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8024152756702073\n","Train loss: 0.49041788280010223\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.29s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8068854324734446\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.446576205348941\n","Weighted F1 Score: 0.7207569676769022\n","Accuracy score: 0.8069335103860381 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 32\n","Seed value: 2022\n","Train loss: 0.619285374879837\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:48, 24.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.5126817226409912\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:48<00:24, 24.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.4561086446046829\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:12<00:00, 24.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069802731411229\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.446576205348941\n","Weighted F1 Score: 0.7207569676769022\n","Accuracy score: 0.8069335103860381 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.7207663446639659\n","The Average  Macro F1-Score of the Language  English is: 0.4465820152675243\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 64\n","Seed value: 2018\n","Train loss: 0.7115506231784821\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:49, 24.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8044258978249874\n","Train loss: 0.5111775249242783\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:49<00:24, 24.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.80699291856348\n","Train loss: 0.44186533242464066\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069802731411229\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 64\n","Seed value: 2019\n","Train loss: 0.6426061540842056\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:49, 24.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.5351106598973274\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:49<00:24, 24.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.4450599402189255\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069613050075873\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 64\n","Seed value: 2020\n","Train loss: 0.9408404678106308\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:49, 24.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.19339276681841175\n","Train loss: 0.7207417786121368\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:49<00:24, 24.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7951251896813353\n","Train loss: 0.609770193696022\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8041350531107738\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44579073251504714\n","Weighted F1 Score: 0.7194892444727347\n","Accuracy score: 0.8043725694773783 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 64\n","Seed value: 2021\n","Train loss: 0.6817275583744049\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:49, 24.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8066009104704096\n","Train loss: 0.573964037001133\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:49<00:24, 24.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.47324303537607193\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 64\n","Seed value: 2022\n","Train loss: 0.5694792568683624\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:24<00:49, 24.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.474591888487339\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:49<00:24, 24.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.46519477665424347\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:13<00:00, 24.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069676277187657\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.720522177010196\n","The Average  Macro F1-Score of the Language  English is: 0.4464307306193288\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 128\n","Seed value: 2018\n","Train loss: 0.6359152272343636\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:25<00:50, 25.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.5127941723912954\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:50<00:25, 25.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.49774531088769436\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:15<00:00, 25.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 128\n","Seed value: 2019\n","Train loss: 0.684056006371975\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:25<00:50, 25.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069802731411229\n","Train loss: 0.49259624630212784\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:50<00:25, 25.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.46320628374814987\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:15<00:00, 25.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069802731411229\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 128\n","Seed value: 2020\n","Train loss: 0.8494908958673477\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:25<00:50, 25.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.26984066767830045\n","Train loss: 0.6163941696286201\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:50<00:25, 25.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8040402124430955\n","Train loss: 0.5028543435037136\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:15<00:00, 25.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069549822964086\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 128\n","Seed value: 2021\n","Train loss: 0.6199037060141563\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:25<00:50, 25.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069802731411229\n","Train loss: 0.4944848082959652\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:50<00:25, 25.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.4576500430703163\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:15<00:00, 25.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 128\n","Seed value: 2022\n","Train loss: 0.5141502618789673\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:25<00:50, 25.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069613050075873\n","Train loss: 0.4454524926841259\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:50<00:25, 25.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.807163631765301\n","Train loss: 0.3759736381471157\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:15<00:00, 25.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8091679312089024\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.47823961038388557\n","Weighted F1 Score: 0.7336494859052567\n","Accuracy score: 0.8099212747794745 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.7233542252967006\n","The Average  Macro F1-Score of the Language  English is: 0.45292050619309643\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 256\n","Seed value: 2018\n","Train loss: 0.6375926714390516\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:26<00:52, 26.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.48619673773646355\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:52<00:26, 26.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069676277187657\n","Train loss: 0.44688101578503847\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:18<00:00, 26.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8076378351036925\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44882733032694433\n","Weighted F1 Score: 0.7216841951897373\n","Accuracy score: 0.8071706345442474 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 256\n","Seed value: 2019\n","Train loss: 0.5096036065369844\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:26<00:52, 26.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.4686730206012726\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:52<00:26, 26.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.44172619190067053\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:18<00:00, 26.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8067716236722307\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.45057027288727\n","Weighted F1 Score: 0.7224043200561124\n","Accuracy score: 0.8073603338708147 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 256\n","Seed value: 2020\n","Train loss: 0.5774361547082663\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:26<00:52, 26.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069676277187657\n","Train loss: 0.5036590602248907\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:52<00:26, 26.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069992412746586\n","Train loss: 0.49005321599543095\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:18<00:00, 26.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 256\n","Seed value: 2021\n","Train loss: 0.543737506493926\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:26<00:52, 26.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.47732597403228283\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:52<00:26, 26.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n","Train loss: 0.4509029798209667\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:18<00:00, 26.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069739504299444\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 256\n","Seed value: 2022\n","Train loss: 0.5944087002426386\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:26<00:52, 26.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069865958523015\n","Train loss: 0.48949686251580715\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:52<00:26, 26.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069613050075873\n","Train loss: 0.4417425664141774\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:18<00:00, 26.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8069802731411229\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.4465907301453992\n","Weighted F1 Score: 0.7207804101445615\n","Accuracy score: 0.80698093521768 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.7212859491359068\n","The Average  Macro F1-Score of the Language  English is: 0.4478339587300824\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 73797\n","Seed value: 2018\n","Train loss: 0.3373182622989191\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [10:55<21:50, 655.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8591489630753667\n","Train loss: 0.27370127173834746\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [21:50<10:55, 655.30s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8624683864441073\n","Train loss: 0.2000536209880036\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [32:46<00:00, 655.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8624747091552857\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7568188675349905\n","Weighted F1 Score: 0.8564799221171775\n","Accuracy score: 0.8651712036422271 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 73797\n","Seed value: 2019\n","Train loss: 0.3366703117036546\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [10:55<21:50, 655.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8692842690945878\n","Train loss: 0.2697998459396188\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [21:50<10:55, 655.14s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8735710672736469\n","Train loss: 0.19448424941451714\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [32:46<00:00, 655.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8637898330804248\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7680278460883665\n","Weighted F1 Score: 0.859515409822528\n","Accuracy score: 0.8637484586929717 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 73797\n","Seed value: 2020\n","Train loss: 0.3402275484941178\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [10:55<21:51, 655.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.866167172483561\n","Train loss: 0.27801150006419556\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [21:51<10:55, 655.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8694739504299444\n","Train loss: 0.20649719456656507\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [32:46<00:00, 655.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8621838644410723\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7729272315691139\n","Weighted F1 Score: 0.8628064979092723\n","Accuracy score: 0.8673053210661102 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 73797\n","Seed value: 2021\n","Train loss: 0.3375951662367307\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [10:55<21:50, 655.01s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8685318664643399\n","Train loss: 0.2728082306792892\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [21:49<10:54, 654.80s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.869663631765301\n","Train loss: 0.19889350783565612\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [32:40<00:00, 653.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8624747091552857\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7610622349783038\n","Weighted F1 Score: 0.8550534674563096\n","Accuracy score: 0.8591482500237124 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: English\n","Sample Size: 73797\n","Seed value: 2022\n","Train loss: 0.3366412019162059\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [10:49<21:38, 649.02s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8626390996459281\n","Train loss: 0.2707253077309502\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [21:39<10:49, 649.33s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.865882650480526\n","Train loss: 0.19470781755417427\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [32:26<00:00, 648.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8582005563985837\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7658835518923394\n","Weighted F1 Score: 0.8574328750006363\n","Accuracy score: 0.8608555439628189 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.8582576344611847\n","The Average  Macro F1-Score of the Language  English is: 0.7649439464126229\n","========================================================================================================================================================================================================\n"," For Sample Size 16 Average Weighted F1-Score 0.7198692036802521 and Average Macro F1-Score 0.44847888881813097 of English \n","\n"," For Sample Size 32 Average Weighted F1-Score 0.7207663446639659 and Average Macro F1-Score 0.4465820152675243 of English \n","\n"," For Sample Size 64 Average Weighted F1-Score 0.720522177010196 and Average Macro F1-Score 0.4464307306193288 of English \n","\n"," For Sample Size 128 Average Weighted F1-Score 0.7233542252967006 and Average Macro F1-Score 0.45292050619309643 of English \n","\n"," For Sample Size 256 Average Weighted F1-Score 0.7212859491359068 and Average Macro F1-Score 0.4478339587300824 of English \n","\n"," For Sample Size 73797 Average Weighted F1-Score 0.8582576344611847 and Average Macro F1-Score 0.7649439464126229 of English \n","\n","====================================================================================================English====================================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wey_mbaAC7F5","executionInfo":{"status":"ok","timestamp":1607253797168,"user_tz":300,"elapsed":13861978,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":[""],"execution_count":10,"outputs":[]}]}