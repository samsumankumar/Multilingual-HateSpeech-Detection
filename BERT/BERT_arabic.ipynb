{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_arabic.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyP7MebVKKxARb7SVx3k7TG9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fvy2c887aK4y","executionInfo":{"status":"ok","timestamp":1607218462628,"user_tz":300,"elapsed":733,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"af8821f1-fdf3-41f5-95f1-f73746a1cb68"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nU58fyX3aWYT","executionInfo":{"status":"ok","timestamp":1607218465184,"user_tz":300,"elapsed":3277,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"d93747ee-74ef-458f-84f8-f44e69611fd9"},"source":["!pip install transformers"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGpvl8-iBLSi","executionInfo":{"status":"ok","timestamp":1607218467606,"user_tz":300,"elapsed":5692,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"64e03e02-be8b-4a82-948d-5dd013c4d2f0"},"source":["!pip install emoji"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"4KyBcSEbadZQ","executionInfo":{"status":"ok","timestamp":1607218467607,"user_tz":300,"elapsed":5686,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"32ca33b9-5da4-4147-84e4-97b5e559e918"},"source":["import tensorflow as tf\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import BertForSequenceClassification, BertTokenizer\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import os\n","import re\n","import emoji\n","import random\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, TweetTokenizer\n","from nltk.corpus import wordnet, stopwords\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, accuracy_score\n","from statistics import mode\n","\n","\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":22,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla V100-SXM2-16GB'"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"2_52J_OQ2dA6","executionInfo":{"status":"ok","timestamp":1607218467608,"user_tz":300,"elapsed":5678,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def preprocess(df):\n","    \n","    #removes URL\n","    pattern = r'https.?://[^\\s]+[\\s]?'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes usernames/mentions\n","    pattern = r'@[^\\s]+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes emoji and smiley\n","    pattern = re.compile(\"[\"\n","                         u\"\\U0001F600-\\U0001F64F\"\n","                         u\"\\U0001F300-\\U0001F5FF\"\n","                         u\"\\U0001F680-\\U0001F6FF\"\n","                         u\"\\U0001F1E0-\\U0001F1FF\"\n","                         u\"\\U00002500-\\U00002BEF\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U000024C2-\\U0001F251\"\n","                         u\"\\U0001f926-\\U0001f937\"\n","                         u\"\\U00010000-\\U0010ffff\"\n","                         u\"\\u2640-\\u2642\"\n","                         u\"\\u2600-\\u2B55\"\n","                         u\"\\u200d\"\n","                         u\"\\u23cf\"\n","                         u\"\\u23e9\"\n","                         u\"\\u231a\"\n","                         u\"\\ufe0f\"\n","                         u\"\\u3030\"\n","                         \"]+\", flags=re.UNICODE)\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes numbers\n","    pattern = r'\\d+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes punctuation\n","    pattern = r\"[^\\w\\s]\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","\n","    #removes stop words\n","    stop_words = stopwords.words(\"english\")    \n","    remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \")\n","                                              if token not in stop_words])\n","    df[\"tweet\"] = df[\"tweet\"].apply(remove_stop_words)\n","    \n","    #removes extra spaces\n","    pattern = r\"[\\s]+\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","    \n","    return(df)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"ATUTCjFaaknr","executionInfo":{"status":"ok","timestamp":1607218467608,"user_tz":300,"elapsed":5673,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def train_validate_test_split(df,seed, train_percent=.8, validate_percent=.125):\n","  train, test = train_test_split(df, train_size=train_percent, stratify=df['label'], random_state=seed)\n","  train, validate = train_test_split(train, test_size=validate_percent, stratify=train['label'], random_state=seed)\n","  return train, validate, test\n","\n","def sample_data(df,sample,seed):\n","    X_train, _, y_train, _ = train_test_split( df['tweet'], df['label'], train_size=sample, random_state=seed, stratify=df['label'])\n","    return pd.concat([X_train,y_train], axis = 1 )\n","\n","def tokenize_data(df):\n","    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in df['tweet']]\n","    # Tokenize with multilingual BERT tokenizer\n","    tokenizer = BertTokenizer.from_pretrained('asafaya/bert-base-arabic', do_lower_case=True)\n","    #tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    \n","    MAX_LEN = 128\n","\n","    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    # Create attention masks\n","    attention_masks = []\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return input_ids, attention_masks\n","\n","def Data_Loader(inputs_ids, attention_masks, df,batch_size=16): \n","    data = TensorDataset(torch.LongTensor(inputs_ids), torch.LongTensor(attention_masks), torch.LongTensor(df['label'].values))\n","    sampler = RandomSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n","    return dataloader"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHohHSKIaq7b","executionInfo":{"status":"ok","timestamp":1607218467609,"user_tz":300,"elapsed":5666,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def model_train(model, train_dataloader, validation_dataloader):\n","    # BERT training loop\n","    epochs = 3\n","    for _ in trange(epochs, desc=\"Epoch\"):  \n","        # Set our model to training mode\n","        model.train()\n","        # Tracking variables\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        # Train the data for one epoch\n","        for step, batch in enumerate(train_dataloader):\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Clear out the gradients (by default they accumulate)\n","            optimizer.zero_grad()\n","            # Forward pass\n","            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","            loss = outputs[\"loss\"]\n","            # Backward pass\n","            loss.backward()\n","            # Update parameters and take a step using the computed gradient\n","            optimizer.step()\n","            # Update tracking variables\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","\n","        ## VALIDATION\n","\n","        # Put model in evaluation mode\n","        model.eval()\n","        # Tracking variables \n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","            with torch.no_grad():\n","              # Forward pass, calculate logit predictions\n","                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n","            eval_accuracy += tmp_eval_accuracy\n","            nb_eval_steps += 1\n","        validation_accuracy = (eval_accuracy/nb_eval_steps)\n","        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","    return validation_accuracy"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fh9pHbFPax9h","executionInfo":{"status":"ok","timestamp":1607218467609,"user_tz":300,"elapsed":5661,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_test(model,prediction_dataloader):\n","    model.eval()\n","    # Tracking variables \n","    predictions , true_labels = [], []\n","# Predict\n","    for batch in prediction_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        predictions+=list(np.argmax(logits, axis=1).flatten())\n","        true_labels+=list(label_ids.flatten())\n","    test_f1_score = f1_score(true_labels, predictions, average= 'macro')\n","    print(\"Macro F1 Score:\",test_f1_score)\n","    test_f1_wscore = f1_score(true_labels, predictions, average= 'weighted')\n","    print(\"Weighted F1 Score:\",test_f1_wscore)\n","    test_accuracy_score = accuracy_score(true_labels, predictions)\n","    print(\"Accuracy score:\", test_accuracy_score, \"\\n\")\n","    print(\"=\"*100)\n","    return test_f1_score, test_f1_wscore"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"C84QhrtEa5xO","executionInfo":{"status":"ok","timestamp":1607218467610,"user_tz":300,"elapsed":5657,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":["def model_initialise():\n","  # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n","  model = BertForSequenceClassification.from_pretrained(\"asafaya/bert-base-arabic\", num_labels=2).cuda()\n","  optimizer = optim.AdamW(params = model.parameters(), lr=2e-5)\n","  return model, optimizer"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxfLlgbVCCBv","executionInfo":{"status":"ok","timestamp":1607219544459,"user_tz":300,"elapsed":1082500,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}},"outputId":"c4200758-047e-4cd9-f597-afa47aa7efda"},"source":["print_stmts= []\n","languages = ['Arabic']\n","directory = 'drive/My Drive/CS695'\n","for lang in languages:\n","  df = pd.read_csv(os.path.join(directory, lang+'.csv'))\n","  df = preprocess(df)\n","  sample_sizes = [16, 32, 64, 128, 256]\n","for sample in sample_sizes: \n","  seeds = [2018,2019, 2020, 2021, 2022]\n","  weighted = []\n","  macro = []\n","  for seed in seeds:\n","    np.random.seed(seed)\n","    train_df, validation_df, test_df = train_validate_test_split(df, seed)\n","    train_len = len(train_df)\n","    if sample==256 and seed==2022:\n","        sample_sizes.append(train_len)\n","    if sample == train_len and seed == 2022:\n","        sample_sizes.remove(train_len)\n","    model, optimizer = model_initialise()\n","    if(sample != train_len):  \n","      train_df_sample = sample_data(train_df,sample,seed)\n","      train_input_ids, train_attention_masks = tokenize_data(train_df_sample)\n","      train_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_df_sample)\n","    else:\n","      train_input_ids, train_attention_masks = tokenize_data(train_df)\n","      train_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_df)\n","\n","    validation_input_ids, validation_attention_masks = tokenize_data(validation_df)\n","    validation_dataloader = Data_Loader(validation_input_ids, validation_attention_masks, validation_df)\n","    print(\"\\nModel Summary:\")\n","    print('Language:', lang)\n","    print('Sample Size:', sample)\n","    print('Seed value:', seed)\n","    validation_accuracy = model_train(model, train_dataloader, validation_dataloader)\n","    test_input_ids, test_attention_masks = tokenize_data(test_df)\n","    test_dataloader = Data_Loader(test_input_ids, test_attention_masks, test_df)\n","    m, w = model_test(model, test_dataloader)\n","    weighted.append(w)\n","    macro.append(m)\n","  print(\"The Average  Weighted F1-Score of the Language \", lang, \"is:\",sum(weighted)/ len(weighted))\n","  print(\"The Average  Macro F1-Score of the Language \", lang, \"is:\",sum(macro)/ len(macro))\n","  print(\"=\"*200)\n","  print_stmts.append(\"For Sample Size \"+str(sample)+\" Average Weighted F1-Score \"+str(sum(weighted)/len(weighted))+\" and Average Macro F1-Score \"+str(sum(macro)/len(macro))+\" of \"+ str(lang))\n","for i in print_stmts:\n","  print(i,\"\\n\")\n","print(\"=\"*100+str(lang)+\"=\"*100)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 16\n","Seed value: 2018\n","Train loss: 0.761111319065094\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:02,  1.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7381756756756757\n","Train loss: 0.6452572345733643\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.5573105812072754\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 16\n","Seed value: 2019\n","Train loss: 0.7817266583442688\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7331081081081081\n","Train loss: 0.6372336149215698\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7775900900900901\n","Train loss: 0.5595855116844177\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 16\n","Seed value: 2020\n","Train loss: 0.7075396776199341\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:02,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7849099099099099\n","Train loss: 0.5729531049728394\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:02<00:01,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n","Train loss: 0.4702599346637726\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 16\n","Seed value: 2021\n","Train loss: 0.7206088304519653\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:02,  1.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7680180180180181\n","Train loss: 0.5708395838737488\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:02<00:01,  1.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.5182000994682312\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 16\n","Seed value: 2022\n","Train loss: 0.6974090933799744\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:02,  1.49s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7612612612612613\n","Train loss: 0.6282309889793396\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:02<00:01,  1.49s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.5451418161392212\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.49s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.6951569873964752\n","The Average  Macro F1-Score of the Language  Arabic is: 0.44084983099951713\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 32\n","Seed value: 2018\n","Train loss: 0.8136326372623444\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7719594594594594\n","Train loss: 0.5435575544834137\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.47860686480998993\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 32\n","Seed value: 2019\n","Train loss: 0.5473939180374146\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.63s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n","Train loss: 0.48574428260326385\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4294455647468567\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 32\n","Seed value: 2020\n","Train loss: 0.6566843092441559\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.536289632320404\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.64s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.500442236661911\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.64s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 32\n","Seed value: 2021\n","Train loss: 0.6693314909934998\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.5254843533039093\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.5027865469455719\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 32\n","Seed value: 2022\n","Train loss: 0.6926472783088684\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.5568088293075562\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4690292477607727\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:04<00:00,  1.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.6951569873964752\n","The Average  Macro F1-Score of the Language  Arabic is: 0.44084983099951713\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 64\n","Seed value: 2018\n","Train loss: 0.6064631268382072\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.92s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n","Train loss: 0.44822418689727783\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7882882882882883\n","Train loss: 0.40673868730664253\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8040540540540541\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.5192837735512894\n","Weighted F1 Score: 0.730734328430774\n","Accuracy score: 0.7987910189982729 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 64\n","Seed value: 2019\n","Train loss: 0.6704947203397751\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.5244077816605568\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.5055199563503265\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 64\n","Seed value: 2020\n","Train loss: 0.5313308015465736\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.4785248041152954\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.4411855638027191\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7972972972972973\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.47820707668573437\n","Weighted F1 Score: 0.7131844035846163\n","Accuracy score: 0.7962003454231433 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 64\n","Seed value: 2021\n","Train loss: 0.6452061533927917\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.92s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4854462221264839\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.91s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4699380025267601\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:05<00:00,  1.90s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 64\n","Seed value: 2022\n","Train loss: 0.5587659105658531\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:01<00:03,  1.89s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.47818339616060257\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:03<00:01,  1.89s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.4531751722097397\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:05<00:00,  1.89s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.7058779388409632\n","The Average  Macro F1-Score of the Language  Arabic is: 0.464008068647115\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 128\n","Seed value: 2018\n","Train loss: 0.5625194013118744\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:02<00:04,  2.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4856507331132889\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:04<00:02,  2.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.4004993662238121\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:07<00:00,  2.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7849099099099099\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.44084983099951713\n","Weighted F1 Score: 0.6951569873964752\n","Accuracy score: 0.7884283246977547 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 128\n","Seed value: 2019\n","Train loss: 0.5302252992987633\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:02<00:04,  2.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4433499239385128\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:04<00:02,  2.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7916666666666667\n","Train loss: 0.36276644468307495\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8429054054054054\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7087949176096884\n","Weighted F1 Score: 0.8267315818041485\n","Accuracy score: 0.8523316062176166 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 128\n","Seed value: 2020\n","Train loss: 0.5565903149545193\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:02<00:04,  2.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.4552023224532604\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:04<00:02,  2.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n","Train loss: 0.36934826150536537\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:07<00:00,  2.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.838963963963964\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7252939270505461\n","Weighted F1 Score: 0.8355906593450518\n","Accuracy score: 0.8583765112262521 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 128\n","Seed value: 2021\n","Train loss: 0.5865986682474613\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:02<00:04,  2.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.450510211288929\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:04<00:02,  2.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.37044027261435986\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:07<00:00,  2.46s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7961711711711711\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.5063039295708955\n","Weighted F1 Score: 0.7251854116923478\n","Accuracy score: 0.7979274611398963 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 128\n","Seed value: 2022\n","Train loss: 0.6174258254468441\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:02<00:05,  2.51s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.48135869577527046\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:04<00:02,  2.49s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.44173618219792843\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7905405405405406\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.45476529482224787\n","Weighted F1 Score: 0.699212651166538\n","Accuracy score: 0.7841105354058722 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.7563754582809123\n","The Average  Macro F1-Score of the Language  Arabic is: 0.5672015800105791\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 256\n","Seed value: 2018\n","Train loss: 0.5275416560471058\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:03<00:07,  3.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.3832092210650444\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:07<00:03,  3.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8710585585585585\n","Train loss: 0.2100411329884082\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:10<00:00,  3.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8699324324324325\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7990940258763402\n","Weighted F1 Score: 0.8660483010008556\n","Accuracy score: 0.8661485319516408 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 256\n","Seed value: 2019\n","Train loss: 0.4964395686984062\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:03<00:07,  3.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7792792792792792\n","Train loss: 0.3910885863006115\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:07<00:03,  3.59s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8260135135135135\n","Train loss: 0.2494367053732276\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:10<00:00,  3.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8507882882882883\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7211878009630819\n","Weighted F1 Score: 0.83257761624764\n","Accuracy score: 0.8549222797927462 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 256\n","Seed value: 2020\n","Train loss: 0.5055221300572157\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:03<00:07,  3.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.34422988072037697\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:07<00:03,  3.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8513513513513513\n","Train loss: 0.20258108410052955\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:10<00:00,  3.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8665540540540541\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7920313548175811\n","Weighted F1 Score: 0.8711812902417521\n","Accuracy score: 0.8825561312607945 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 256\n","Seed value: 2021\n","Train loss: 0.523699801415205\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:03<00:07,  3.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.414869237691164\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:07<00:03,  3.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8733108108108109\n","Train loss: 0.22233695397153497\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:10<00:00,  3.55s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8614864864864865\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7897845948820462\n","Weighted F1 Score: 0.8563856329662546\n","Accuracy score: 0.853195164075993 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 256\n","Seed value: 2022\n","Train loss: 0.5293125659227371\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:03<00:07,  3.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.793918918918919\n","Train loss: 0.4649667739868164\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [00:07<00:03,  3.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8141891891891891\n","Train loss: 0.37047802563756704\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [00:10<00:00,  3.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8175675675675675\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.7959571704290743\n","Weighted F1 Score: 0.8573499956895887\n","Accuracy score: 0.8514680483592401 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.8567085672292182\n","The Average  Macro F1-Score of the Language  Arabic is: 0.7796109893936248\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 4051\n","Seed value: 2018\n","Train loss: 0.32277468149000266\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:36<01:12, 36.34s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9037162162162162\n","Train loss: 0.1990095108498152\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [01:12<00:36, 36.30s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9155405405405406\n","Train loss: 0.10049040190046873\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:48<00:00, 36.32s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8952702702702703\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.8543518363239979\n","Weighted F1 Score: 0.8994807356939469\n","Accuracy score: 0.8963730569948186 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 4051\n","Seed value: 2019\n","Train loss: 0.30568578316441436\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:36<01:12, 36.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9003378378378378\n","Train loss: 0.1684904418863708\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [01:12<00:36, 36.45s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9121621621621622\n","Train loss: 0.08720334098623024\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:49<00:00, 36.50s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9087837837837838\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.8677484109867037\n","Weighted F1 Score: 0.910966352943638\n","Accuracy score: 0.9101899827288429 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 4051\n","Seed value: 2020\n","Train loss: 0.3241280182374744\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:36<01:13, 36.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9014639639639639\n","Train loss: 0.18614191902933394\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [01:12<00:36, 36.50s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.918918918918919\n","Train loss: 0.08756070838018327\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:49<00:00, 36.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9054054054054054\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.8829184896503548\n","Weighted F1 Score: 0.9229287866864455\n","Accuracy score: 0.924006908462867 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 4051\n","Seed value: 2021\n","Train loss: 0.3184416338684052\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:36<01:13, 36.71s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8817567567567568\n","Train loss: 0.181548639501291\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [01:13<00:36, 36.68s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8918918918918919\n","Train loss: 0.10963626680840687\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:50<00:00, 36.69s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8913288288288288\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.8505152025080105\n","Weighted F1 Score: 0.9017476152875917\n","Accuracy score: 0.9032815198618307 \n","\n","====================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","Model Summary:\n","Language: Arabic\n","Sample Size: 4051\n","Seed value: 2022\n","Train loss: 0.3104409219433121\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  33%|███▎      | 1/3 [00:36<01:12, 36.41s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8941441441441442\n","Train loss: 0.17226482951998945\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  67%|██████▋   | 2/3 [01:12<00:36, 36.40s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8811936936936936\n","Train loss: 0.09850293807740375\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 3/3 [01:49<00:00, 36.41s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.893581081081081\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Macro F1 Score: 0.8471050486930947\n","Weighted F1 Score: 0.8988968959624088\n","Accuracy score: 0.8998272884283247 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.9068040773148063\n","The Average  Macro F1-Score of the Language  Arabic is: 0.8605277976324324\n","========================================================================================================================================================================================================\n","For Sample Size 16 Average Weighted F1-Score 0.6951569873964752 and Average Macro F1-Score 0.44084983099951713 of Arabic \n","\n","For Sample Size 32 Average Weighted F1-Score 0.6951569873964752 and Average Macro F1-Score 0.44084983099951713 of Arabic \n","\n","For Sample Size 64 Average Weighted F1-Score 0.7058779388409632 and Average Macro F1-Score 0.464008068647115 of Arabic \n","\n","For Sample Size 128 Average Weighted F1-Score 0.7563754582809123 and Average Macro F1-Score 0.5672015800105791 of Arabic \n","\n","For Sample Size 256 Average Weighted F1-Score 0.8567085672292182 and Average Macro F1-Score 0.7796109893936248 of Arabic \n","\n","For Sample Size 4051 Average Weighted F1-Score 0.9068040773148063 and Average Macro F1-Score 0.8605277976324324 of Arabic \n","\n","====================================================================================================Arabic====================================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wey_mbaAC7F5","executionInfo":{"status":"ok","timestamp":1607219544460,"user_tz":300,"elapsed":1082496,"user":{"displayName":"Sreenitha Kasarapu","photoUrl":"","userId":"11308003982662682363"}}},"source":[""],"execution_count":28,"outputs":[]}]}