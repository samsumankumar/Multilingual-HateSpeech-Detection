{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"XLMR_multilingual.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"19d0bdd8bde54a8bb9b441a6c67fee33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7c9e2e5035334dc3990d86b89ee7fedb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e81de779c20846e28277ddaf8a721a6a","IPY_MODEL_de299a01b7c24c51a8ea5648c531fdc6"]}},"7c9e2e5035334dc3990d86b89ee7fedb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e81de779c20846e28277ddaf8a721a6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c189186597b24c7389f6ae629d50a463","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3c3f05909f6e4659a89c8a4c3cee5c0a"}},"de299a01b7c24c51a8ea5648c531fdc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6094e544f9cd4f5692f061ca06974c34","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.07M/5.07M [00:01&lt;00:00, 3.25MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_87bc1c15975c47a9bd4c596835757c1c"}},"c189186597b24c7389f6ae629d50a463":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3c3f05909f6e4659a89c8a4c3cee5c0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6094e544f9cd4f5692f061ca06974c34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"87bc1c15975c47a9bd4c596835757c1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ff0f7d388cb8444693c5125c154065b4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_93d1f19ac0e44f9b80d98e00feb83556","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_afa2461ee3744237b87242c117665789","IPY_MODEL_883ed162b2f84a8e92f577ee0051dde6"]}},"93d1f19ac0e44f9b80d98e00feb83556":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"afa2461ee3744237b87242c117665789":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1eeab9ec2f054d6fb571582de447050a","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":512,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":512,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2e8758e94da4207b75419ac70fe8bbf"}},"883ed162b2f84a8e92f577ee0051dde6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9f7826a467a84099bfe1e87a0a9d1132","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 512/512 [00:00&lt;00:00, 1.26kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ea5d69fa26444d00ab4df5c7f389575c"}},"1eeab9ec2f054d6fb571582de447050a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b2e8758e94da4207b75419ac70fe8bbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f7826a467a84099bfe1e87a0a9d1132":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ea5d69fa26444d00ab4df5c7f389575c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2938e57e30494736a9a7b16f57e04684":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_035b3d509bf7410c845154aaf5679272","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_16f4ffc489864e3b9b422a49d23eff8a","IPY_MODEL_4039d44dcf1c4b0fa721cd8870d9cd10"]}},"035b3d509bf7410c845154aaf5679272":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"16f4ffc489864e3b9b422a49d23eff8a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_596a1b5af6104826b7c844b6c2d703e8","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1115590446,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1115590446,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a827b549ec64dbb9534a164eb09fd68"}},"4039d44dcf1c4b0fa721cd8870d9cd10":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4fda2457878d408a98eebda0e6d72e6c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.12G/1.12G [00:16&lt;00:00, 67.5MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_30f95e41f3a4466aa310e48e3186db25"}},"596a1b5af6104826b7c844b6c2d703e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6a827b549ec64dbb9534a164eb09fd68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fda2457878d408a98eebda0e6d72e6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"30f95e41f3a4466aa310e48e3186db25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"-u2JgtiSYQiO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606264424264,"user_tz":300,"elapsed":21769,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"212938ae-d8de-4ef7-b981-d021c87dab5e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hG52WywReczh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606264372548,"user_tz":300,"elapsed":8505,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"bcc760c0-9cc7-42a2-a0cb-75ff1f823a0f"},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\r\u001b[K     |▎                               | 10kB 20.3MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.2MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 26.5MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 18.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 17.2MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 18.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 14.5MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 15.0MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 14.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 14.0MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 14.0MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 14.0MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 14.0MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 14.0MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 47.8MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 59.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 57.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b15cc9566a83f4317156be1ffcb1ad266f2364588171dfaf76ff1b568c7af633\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zc3H9ri4tpt7","executionInfo":{"status":"ok","timestamp":1606264375818,"user_tz":300,"elapsed":11262,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"7ef0bf78-81e6-4667-e481-a35c946b0e4a"},"source":["!pip install emoji"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n","\r\u001b[K     |██████▍                         | 10kB 16.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 20.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 15.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=4abd722f1f4caa7d7e4e326c79ebdec34d9ae77ef8f3043dfa24b7c8cc3ce623\n","  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7A8Wt1WgeMUz","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1606264381758,"user_tz":300,"elapsed":16544,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"d44d6cf5-cab2-43ba-d6c4-670164d41c3a"},"source":["import warnings\n","\n","warnings.simplefilter(\"ignore\", UserWarning)\n","warnings.simplefilter(\"ignore\", FutureWarning)\n","warnings.simplefilter(\"ignore\", DeprecationWarning)\n","\n","import tensorflow as tf\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import os\n","import numpy as np\n","from sklearn.metrics import f1_score, accuracy_score\n","from statistics import mode\n","\n","import re\n","import emoji\n","import random\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, TweetTokenizer\n","from nltk.corpus import wordnet, stopwords\n","from imblearn.over_sampling import SMOTE\n","\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla V100-SXM2-16GB'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"BpztF_TVuPTx","executionInfo":{"status":"ok","timestamp":1606264429659,"user_tz":300,"elapsed":1357,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def preprocess(df):\n","    \n","    #removes URL\n","    pattern = r'https.?://[^\\s]+[\\s]?'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes usernames/mentions\n","    pattern = r'@[^\\s]+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes emoji and smiley\n","    pattern = re.compile(\"[\"\n","                         u\"\\U0001F600-\\U0001F64F\"\n","                         u\"\\U0001F300-\\U0001F5FF\"\n","                         u\"\\U0001F680-\\U0001F6FF\"\n","                         u\"\\U0001F1E0-\\U0001F1FF\"\n","                         u\"\\U00002500-\\U00002BEF\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U000024C2-\\U0001F251\"\n","                         u\"\\U0001f926-\\U0001f937\"\n","                         u\"\\U00010000-\\U0010ffff\"\n","                         u\"\\u2640-\\u2642\"\n","                         u\"\\u2600-\\u2B55\"\n","                         u\"\\u200d\"\n","                         u\"\\u23cf\"\n","                         u\"\\u23e9\"\n","                         u\"\\u231a\"\n","                         u\"\\ufe0f\"\n","                         u\"\\u3030\"\n","                         \"]+\", flags=re.UNICODE)\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes numbers\n","    pattern = r'\\d+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes punctuation\n","    pattern = r\"[^\\w\\s]\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","\n","    #removes stop words\n","    stop_words = stopwords.words(\"english\")    \n","    remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \")\n","                                              if token not in stop_words])\n","    df[\"tweet\"] = df[\"tweet\"].apply(remove_stop_words)\n","    \n","    #removes extra spaces\n","    pattern = r\"[\\s]+\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","    \n","    return(df)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"w264IlP9ghob","executionInfo":{"status":"ok","timestamp":1606264429663,"user_tz":300,"elapsed":1160,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def train_validate_split(df,seed=42,validate_percent = 0.1):\n","  train, validate = train_test_split(df, test_size=validate_percent, random_state=seed, stratify=df['label'])\n","  return train, validate\n","\n","def train_validate_test_split(df,seed, train_percent=.8, validate_percent=.125):\n","  train, test = train_test_split(df, train_size=train_percent, random_state=seed, stratify=df['label'])\n","  train, validate = train_test_split(train, test_size=validate_percent, random_state=seed, stratify=train['label'])\n","  return train, validate, test\n","\n","def tokenize_data(df):\n","    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in df['tweet']]\n","    # Tokenize with multilingual BERT tokenizer\n","    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    MAX_LEN = 128\n","\n","    # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    labels = df['label'].copy()\n","    # Create attention masks\n","    attention_masks = []\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return input_ids, attention_masks, labels\n","\n","def Data_Loader(inputs_ids, attention_masks, labels, batch_size=16): \n","    data = TensorDataset(torch.LongTensor(inputs_ids), torch.LongTensor(attention_masks), torch.LongTensor(labels.ravel()))\n","    sampler = RandomSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n","    return dataloader"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRd06Ne4ghq5","executionInfo":{"status":"ok","timestamp":1606264429668,"user_tz":300,"elapsed":987,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def model_train(model, train_dataloader, validation_dataloader):\n","    # BERT training loop\n","    epochs = 5\n","    for _ in trange(epochs, desc=\"Epoch\"):  \n","        # Set our model to training mode\n","        model.train()\n","        # Tracking variables\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        # Train the data for one epoch\n","        for step, batch in enumerate(train_dataloader):\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Clear out the gradients (by default they accumulate)\n","            optimizer.zero_grad()\n","            # Forward pass\n","            loss,_ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","            # Backward pass\n","            loss.backward()\n","            # Update parameters and take a step using the computed gradient\n","            optimizer.step()\n","            scheduler.step()\n","            # Update tracking variables\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","\n","        ## VALIDATION\n","\n","        # Put model in evaluation mode\n","        model.eval()\n","        # Tracking variables \n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            # Add batch to GPU\n","            batch = tuple(t.to(device) for t in batch)\n","            # Unpack the inputs from our dataloader\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","            with torch.no_grad():\n","                # Forward pass, calculate logit predictions\n","                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n","            eval_accuracy += tmp_eval_accuracy\n","            nb_eval_steps += 1\n","        validation_accuracy = (eval_accuracy/nb_eval_steps)\n","        print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","    return validation_accuracy"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsnuBqImghv2","executionInfo":{"status":"ok","timestamp":1606264430044,"user_tz":300,"elapsed":750,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_test(model,prediction_dataloader):\n","    model.eval()\n","    # Tracking variables \n","    predictions , true_labels = [], []\n","# Predict \n","    for batch in prediction_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        predictions+=list(np.argmax(logits, axis=1).flatten())\n","        true_labels+=list(label_ids.flatten())\n","    weighted_f1_score = f1_score(true_labels, predictions, average= 'weighted')\n","    print(\"Weighted F1 Score:\",weighted_f1_score)\n","    macro_f1_score = f1_score(true_labels, predictions, average= 'macro')\n","    print(\"Macro F1 Score:\",macro_f1_score)    \n","    test_accuracy_score = accuracy_score(true_labels, predictions)\n","    print(\"Accuracy score:\", test_accuracy_score, \"\\n\")\n","    print(\"=\"*100)\n","    return weighted_f1_score, macro_f1_score"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLCuCukimkNk","executionInfo":{"status":"ok","timestamp":1606264431241,"user_tz":300,"elapsed":1211,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def model_initialise(dataloader_len,path= None , use_saved_model=False):\n","  model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels = 2, output_attentions = False, output_hidden_states = False).cuda()\n","  optimizer = optim.AdamW(params = model.parameters(), lr=2e-5)\n","  epochs = 5\n","  total_steps = dataloader_len * epochs\n","  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n","  \n","  if(use_saved_model==True):\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","  return model, optimizer, scheduler"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBxfdamsdX0r","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["19d0bdd8bde54a8bb9b441a6c67fee33","7c9e2e5035334dc3990d86b89ee7fedb","e81de779c20846e28277ddaf8a721a6a","de299a01b7c24c51a8ea5648c531fdc6","c189186597b24c7389f6ae629d50a463","3c3f05909f6e4659a89c8a4c3cee5c0a","6094e544f9cd4f5692f061ca06974c34","87bc1c15975c47a9bd4c596835757c1c","ff0f7d388cb8444693c5125c154065b4","93d1f19ac0e44f9b80d98e00feb83556","afa2461ee3744237b87242c117665789","883ed162b2f84a8e92f577ee0051dde6","1eeab9ec2f054d6fb571582de447050a","b2e8758e94da4207b75419ac70fe8bbf","9f7826a467a84099bfe1e87a0a9d1132","ea5d69fa26444d00ab4df5c7f389575c","2938e57e30494736a9a7b16f57e04684","035b3d509bf7410c845154aaf5679272","16f4ffc489864e3b9b422a49d23eff8a","4039d44dcf1c4b0fa721cd8870d9cd10","596a1b5af6104826b7c844b6c2d703e8","6a827b549ec64dbb9534a164eb09fd68","4fda2457878d408a98eebda0e6d72e6c","30f95e41f3a4466aa310e48e3186db25"]},"executionInfo":{"status":"ok","timestamp":1606294666785,"user_tz":300,"elapsed":30194548,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"61afe6fe-d739-4f3f-fa2f-8a22e4b8a79d"},"source":["print_stmts= []\n","languages = {'en_fr':'Arabic','fr_ar':'English','en_ar':'French'}\n","directory = './'\n","drive_directory = '/content/drive/My Drive/XLMRMultilingual models'\n","for lang1, lang2 in languages.items():\n","\tdf = pd.read_csv(os.path.join(directory, lang1+'.csv'))\n","\tdf = preprocess(df)\n","\ttrain_df, validation_df = train_validate_split(df)\n","\ttrain_input_ids, train_attention_masks, train_labels = tokenize_data(df)\n","\ttrain_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_labels)\n","\tvalidation_input_ids, validation_attention_masks, validation_labels = tokenize_data(validation_df)\n","\tvalidation_dataloader = Data_Loader(validation_input_ids, validation_attention_masks, validation_labels)\n","\tmodel, optimizer, scheduler = model_initialise(len(train_dataloader))\n","\tvalidation_accuracy = model_train(model, train_dataloader, validation_dataloader)\n","\tfname = 'XLMR'+lang2+'.pth'\n","\tpath = os.path.join(drive_directory, fname)\n","\ttorch.save({'model_state_dict': model.state_dict(),\n","\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict()}, path)\n","\ttest_df = pd.read_csv(os.path.join(directory, lang2+'.csv'))\n","\ttest_input_ids, test_attention_masks, test_labels = tokenize_data(test_df)\n","\ttest_dataloader = Data_Loader(test_input_ids, test_attention_masks, test_labels)\n","\tprint(\"\\nZero Shot Model for test:\",lang2,'\\n')\n","\t_, _ = model_test(model, test_dataloader)\n","\tdf = pd.read_csv(os.path.join(directory, lang2+'.csv'))\n","\tdf = preprocess(df)\n","\tweighted = []\n","\tmacro = []\n","\tseeds = [2018, 2019, 2020, 2021, 2022]\n","\tscores=[]\n","\tfor seed in seeds:\n","\t\tnp.random.seed(seed)\n","\t\ttrain_df, validation_df, test_df = train_validate_test_split(df, seed)\n","\t\ttrain_input_ids, train_attention_masks, train_labels = tokenize_data(train_df)\n","\t\ttrain_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_labels)\n","\t\tvalidation_input_ids, validation_attention_masks, validation_labels = tokenize_data(validation_df)\n","\t\tvalidation_dataloader = Data_Loader(validation_input_ids, validation_attention_masks, validation_labels)\n","\t\tprint(\"\\nModel Summary:\")\n","\t\tprint('Language:', lang2)\n","\t\tprint('Seed value:', seed)\n","\t\tmodel, optimizer, scheduler = model_initialise(len(train_dataloader),path,use_saved_model=True)\n","\t\tvalidation_accuracy = model_train(model, train_dataloader, validation_dataloader)\n","\t\ttest_input_ids, test_attention_masks, test_labels = tokenize_data(test_df)\n","\t\ttest_dataloader = Data_Loader(test_input_ids, test_attention_masks, test_labels)\n","\t\tw, m = model_test(model, test_dataloader)\n","\t\tweighted.append(w)\n","\t\tmacro.append(m)\n","\tprint(\"The Average  Weighted F1-Score of the Language \", lang2, \"is:\",sum(weighted)/ len(weighted))\n","\tprint(\"The Average  Macro F1-Score of the Language \", lang2, \"is:\",sum(macro)/ len(macro))\n","\tprint(\"=\"*200)\n","\tprint_stmts.append(\"Average Weighted F1-Score \"+str(sum(weighted)/len(weighted))+\" and Average Macro F1-Score \"+str(sum(macro)/len(macro))+\" of \"+ str(lang2))\n","for i in print_stmts:\n","\tprint(i,\"\\n\")"],"execution_count":10,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19d0bdd8bde54a8bb9b441a6c67fee33","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff0f7d388cb8444693c5125c154065b4","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=512.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2938e57e30494736a9a7b16f57e04684","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1115590446.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3734834009796569\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [16:39<1:06:38, 999.74s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8605697151424287\n","Train loss: 0.32180686287823346\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [33:17<49:57, 999.17s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8744586040313177\n","Train loss: 0.2944139986020392\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [49:54<33:17, 998.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8913980509745127\n","Train loss: 0.26376109515860824\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [1:06:31<16:38, 998.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9080043311677495\n","Train loss: 0.23477912550828983\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [1:23:08<00:00, 997.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9182179743461603\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Zero Shot Model for test: Arabic \n","\n","Weighted F1 Score: 0.7539611137082988\n","Macro F1 Score: 0.579280906376903\n","Accuracy score: 0.796821008984105 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2018\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.33319818572412563\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:35, 38.78s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.893581081081081\n","Train loss: 0.24456842366869994\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:17<01:55, 38.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.893581081081081\n","Train loss: 0.1924621929583235\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:17, 38.56s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8969594594594594\n","Train loss: 0.13619173324564252\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.50s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9037162162162162\n","Train loss: 0.10047227145707983\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9037162162162162\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.9030220797171122\n","Macro F1 Score: 0.8530112353126162\n","Accuracy score: 0.9041450777202072 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2019\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3348154091078231\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8902027027027027\n","Train loss: 0.23595602606518531\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.902027027027027\n","Train loss: 0.16212986346598215\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8806306306306307\n","Train loss: 0.11001020638587496\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8969594594594594\n","Train loss: 0.06901852041207929\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8969594594594594\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8951135569321306\n","Macro F1 Score: 0.8422156784360977\n","Accuracy score: 0.8955094991364422 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2020\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3392985665833387\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8997747747747747\n","Train loss: 0.23165368424420515\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.910472972972973\n","Train loss: 0.16688190930060984\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8918918918918919\n","Train loss: 0.11207939455318668\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.902027027027027\n","Train loss: 0.07831082947774035\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9121621621621622\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.9152442062445174\n","Macro F1 Score: 0.8727849122932884\n","Accuracy score: 0.9153713298791019 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2021\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3347698391275847\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8952702702702703\n","Train loss: 0.2297513915416528\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8986486486486487\n","Train loss: 0.1544889106825874\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.893581081081081\n","Train loss: 0.10174737859693334\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8868243243243243\n","Train loss: 0.06801824952479393\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8918918918918919\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8946241366100686\n","Macro F1 Score: 0.8383294441452761\n","Accuracy score: 0.8972366148531952 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2022\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3164765451206114\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8811936936936936\n","Train loss: 0.2179719464941405\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8800675675675675\n","Train loss: 0.15630929098339882\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.893018018018018\n","Train loss: 0.09713975124622101\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.902027027027027\n","Train loss: 0.06295521385549736\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9003378378378378\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.891399653967264\n","Macro F1 Score: 0.8350215081436817\n","Accuracy score: 0.8929188255613126 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.8998807266942185\n","The Average  Macro F1-Score of the Language  Arabic is: 0.8482725556661921\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.5197336050813601\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [01:05<04:22, 65.72s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8225524475524476\n","Train loss: 0.37277104894388213\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [02:11<03:16, 65.67s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.875\n","Train loss: 0.2984921696880773\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [03:16<02:11, 65.63s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9155375874125874\n","Train loss: 0.24781449089787866\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [04:22<01:05, 65.60s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9332386363636364\n","Train loss: 0.2031506095761986\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [05:27<00:00, 65.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9340034965034966\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Zero Shot Model for test: English \n","\n","Weighted F1 Score: 0.7568607824813645\n","Macro F1 Score: 0.5590009465496957\n","Accuracy score: 0.7945288638476278 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2018\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.36120438524302834\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:36<46:24, 696.05s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8508093070308548\n","Train loss: 0.31638229705365856\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:12<34:48, 696.23s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8565882650480526\n","Train loss: 0.2834163580750798\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:50<23:13, 696.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8612417804754678\n","Train loss: 0.24601886915670876\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:27<11:36, 696.88s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8590477996965098\n","Train loss: 0.2116280013976795\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:04<00:00, 696.87s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8616021750126455\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8611457140498046\n","Macro F1 Score: 0.7736580630256132\n","Accuracy score: 0.8633690600398368 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2019\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3881661957156082\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:36<46:25, 696.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8485078401618613\n","Train loss: 0.3237153235423172\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:13<34:49, 696.64s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.868063985837127\n","Train loss: 0.2905816528316262\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:50<23:13, 696.65s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8712000505816894\n","Train loss: 0.2585893683648312\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:26<11:36, 696.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8652250885179564\n","Train loss: 0.2260776189386638\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:03<00:00, 696.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.868715225088518\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8601001324082551\n","Macro F1 Score: 0.7737363834608356\n","Accuracy score: 0.8611875177843119 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2020\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.35840046078607896\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:37<46:29, 697.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8592501264542235\n","Train loss: 0.3137555369970411\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:14<34:51, 697.26s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8637013151239251\n","Train loss: 0.31825844710622103\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:50<23:13, 696.92s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8586747597369753\n","Train loss: 0.2708698518359426\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:26<11:36, 696.80s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8665591805766313\n","Train loss: 0.24352293666480435\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:03<00:00, 696.68s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8598191704602933\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8637769310424932\n","Macro F1 Score: 0.7766964910627228\n","Accuracy score: 0.8667836479180498 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2021\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.35709880783535347\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:37<46:28, 697.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8568538189175517\n","Train loss: 0.3194343811056502\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:14<34:51, 697.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8671029337379869\n","Train loss: 0.2848335175358894\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:51<23:14, 697.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8650227617602427\n","Train loss: 0.24903473242715166\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:26<11:36, 696.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.866831057157309\n","Train loss: 0.21495501431375064\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:02<00:00, 696.47s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8689997470915529\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.857066481971672\n","Macro F1 Score: 0.7660983043582515\n","Accuracy score: 0.8599544721616238 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2022\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.358400487027854\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:35<46:22, 695.57s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8569613050075872\n","Train loss: 0.312022206006677\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:11<34:47, 695.69s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8667488619119879\n","Train loss: 0.27882615593095456\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:48<23:12, 696.01s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8677794638340921\n","Train loss: 0.23995585219238866\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:25<11:36, 696.22s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.868259989883662\n","Train loss: 0.203045981643415\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:01<00:00, 696.26s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8654906423874558\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8605045555914574\n","Macro F1 Score: 0.7719418264175988\n","Accuracy score: 0.8631793607132695 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.8605187630127364\n","The Average  Macro F1-Score of the Language  English is: 0.7724262136650044\n","========================================================================================================================================================================================================\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3685951924133457\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [17:19<1:09:16, 1039.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8521012931034483\n","Train loss: 0.319102366432524\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [34:38<51:57, 1039.12s/it]  "],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8750897988505747\n","Train loss: 0.2926776656365672\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [51:57<34:38, 1039.19s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8901760057471264\n","Train loss: 0.2657543064785499\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [1:09:17<17:19, 1039.26s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.905621408045977\n","Train loss: 0.23953153425084012\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [1:26:37<00:00, 1039.42s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.9135237068965517\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Zero Shot Model for test: French \n","\n","Weighted F1 Score: 0.6582832958912695\n","Macro F1 Score: 0.584831632430229\n","Accuracy score: 0.6934426229508197 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2018\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.5960467568150273\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.17s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.653125\n","Train loss: 0.49616401201045074\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.18s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6375\n","Train loss: 0.4077246556127513\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6171875\n","Train loss: 0.29622458921814404\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.5890625\n","Train loss: 0.22328302146935905\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:41<00:00,  8.21s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.68125\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.7303481697527759\n","Macro F1 Score: 0.6950693024312656\n","Accuracy score: 0.7295081967213115 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2019\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.5985802207831983\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.640625\n","Train loss: 0.48058044413725537\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6328125\n","Train loss: 0.35988784098514803\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.625\n","Train loss: 0.25943213659856057\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6671875\n","Train loss: 0.16485585310254935\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6578125\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.7266684429735377\n","Macro F1 Score: 0.6869118905047049\n","Accuracy score: 0.7295081967213115 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2020\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.6177335193863621\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.684375\n","Train loss: 0.48487425567927184\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.65625\n","Train loss: 0.3934462702384702\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.696875\n","Train loss: 0.30564809452604363\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.68125\n","Train loss: 0.23027063657840094\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6984375\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.6744010088272384\n","Macro F1 Score: 0.6246153846153846\n","Accuracy score: 0.680327868852459 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2021\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.596904817002791\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n","Train loss: 0.4992177271180683\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.70625\n","Train loss: 0.3879742696881294\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7046875\n","Train loss: 0.31147828339426603\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7015625\n","Train loss: 0.1961522346569432\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7296875\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.7568531887721858\n","Macro F1 Score: 0.7187599364069952\n","Accuracy score: 0.7622950819672131 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2022\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.5771397291510193\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6125\n","Train loss: 0.4623527571007057\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6484375\n","Train loss: 0.36395569504411135\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.5765625\n","Train loss: 0.20475372199521022\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6140625\n","Train loss: 0.12859782145393114\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.5984375\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.6941114907723125\n","Macro F1 Score: 0.660590087122044\n","Accuracy score: 0.6885245901639344 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  French is: 0.7164764602196101\n","The Average  Macro F1-Score of the Language  French is: 0.6771893202160788\n","========================================================================================================================================================================================================\n","Average Weighted F1-Score 0.8998807266942185 and Average Macro F1-Score 0.8482725556661921 of Arabic \n","\n","Average Weighted F1-Score 0.8605187630127364 and Average Macro F1-Score 0.7724262136650044 of English \n","\n","Average Weighted F1-Score 0.7164764602196101 and Average Macro F1-Score 0.6771893202160788 of French \n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w0TB93dPLiCn"},"source":["# Downloading: 100%\n","# 5.07M/5.07M [00:01<00:00, 3.25MB/s]\n","\n","# Downloading: 100%\n","# 512/512 [00:00<00:00, 1.26kB/s]\n","\n","# Downloading: 100%\n","# 1.12G/1.12G [00:16<00:00, 67.5MB/s]\n","\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3734834009796569\n","# Epoch:  20%|██        | 1/5 [16:39<1:06:38, 999.74s/it]Validation Accuracy: 0.8605697151424287\n","# Train loss: 0.32180686287823346\n","# Epoch:  40%|████      | 2/5 [33:17<49:57, 999.17s/it]  Validation Accuracy: 0.8744586040313177\n","# Train loss: 0.2944139986020392\n","# Epoch:  60%|██████    | 3/5 [49:54<33:17, 998.62s/it]Validation Accuracy: 0.8913980509745127\n","# Train loss: 0.26376109515860824\n","# Epoch:  80%|████████  | 4/5 [1:06:31<16:38, 998.07s/it]Validation Accuracy: 0.9080043311677495\n","# Train loss: 0.23477912550828983\n","# Epoch: 100%|██████████| 5/5 [1:23:08<00:00, 997.65s/it]Validation Accuracy: 0.9182179743461603\n","\n","\n","# Zero Shot Model for test: Arabic \n","\n","# Weighted F1 Score: 0.7539611137082988\n","# Macro F1 Score: 0.579280906376903\n","# Accuracy score: 0.796821008984105 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2018\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.33319818572412563\n","# Epoch:  20%|██        | 1/5 [00:38<02:35, 38.78s/it]Validation Accuracy: 0.893581081081081\n","# Train loss: 0.24456842366869994\n","# Epoch:  40%|████      | 2/5 [01:17<01:55, 38.65s/it]Validation Accuracy: 0.893581081081081\n","# Train loss: 0.1924621929583235\n","# Epoch:  60%|██████    | 3/5 [01:55<01:17, 38.56s/it]Validation Accuracy: 0.8969594594594594\n","# Train loss: 0.13619173324564252\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.50s/it]Validation Accuracy: 0.9037162162162162\n","# Train loss: 0.10047227145707983\n","# Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]Validation Accuracy: 0.9037162162162162\n","\n","# Weighted F1 Score: 0.9030220797171122\n","# Macro F1 Score: 0.8530112353126162\n","# Accuracy score: 0.9041450777202072 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2019\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3348154091078231\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.36s/it]Validation Accuracy: 0.8902027027027027\n","# Train loss: 0.23595602606518531\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.36s/it]Validation Accuracy: 0.902027027027027\n","# Train loss: 0.16212986346598215\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.36s/it]Validation Accuracy: 0.8806306306306307\n","# Train loss: 0.11001020638587496\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.36s/it]Validation Accuracy: 0.8969594594594594\n","# Train loss: 0.06901852041207929\n","# Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.37s/it]Validation Accuracy: 0.8969594594594594\n","\n","# Weighted F1 Score: 0.8951135569321306\n","# Macro F1 Score: 0.8422156784360977\n","# Accuracy score: 0.8955094991364422 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2020\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3392985665833387\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.36s/it]Validation Accuracy: 0.8997747747747747\n","# Train loss: 0.23165368424420515\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.36s/it]Validation Accuracy: 0.910472972972973\n","# Train loss: 0.16688190930060984\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.37s/it]Validation Accuracy: 0.8918918918918919\n","# Train loss: 0.11207939455318668\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.37s/it]Validation Accuracy: 0.902027027027027\n","# Train loss: 0.07831082947774035\n","# Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.37s/it]Validation Accuracy: 0.9121621621621622\n","\n","# Weighted F1 Score: 0.9152442062445174\n","# Macro F1 Score: 0.8727849122932884\n","# Accuracy score: 0.9153713298791019 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2021\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3347698391275847\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.35s/it]Validation Accuracy: 0.8952702702702703\n","# Train loss: 0.2297513915416528\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.35s/it]Validation Accuracy: 0.8986486486486487\n","# Train loss: 0.1544889106825874\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.35s/it]Validation Accuracy: 0.893581081081081\n","# Train loss: 0.10174737859693334\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.35s/it]Validation Accuracy: 0.8868243243243243\n","# Train loss: 0.06801824952479393\n","# Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.35s/it]Validation Accuracy: 0.8918918918918919\n","\n","# Weighted F1 Score: 0.8946241366100686\n","# Macro F1 Score: 0.8383294441452761\n","# Accuracy score: 0.8972366148531952 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2022\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3164765451206114\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.37s/it]Validation Accuracy: 0.8811936936936936\n","# Train loss: 0.2179719464941405\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.37s/it]Validation Accuracy: 0.8800675675675675\n","# Train loss: 0.15630929098339882\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.37s/it]Validation Accuracy: 0.893018018018018\n","# Train loss: 0.09713975124622101\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.36s/it]Validation Accuracy: 0.902027027027027\n","# Train loss: 0.06295521385549736\n","# Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.36s/it]Validation Accuracy: 0.9003378378378378\n","\n","# Weighted F1 Score: 0.891399653967264\n","# Macro F1 Score: 0.8350215081436817\n","# Accuracy score: 0.8929188255613126 \n","\n","# ====================================================================================================\n","# The Average  Weighted F1-Score of the Language  Arabic is: 0.8998807266942185\n","# The Average  Macro F1-Score of the Language  Arabic is: 0.8482725556661921\n","# ========================================================================================================================================================================================================\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.5197336050813601\n","# Epoch:  20%|██        | 1/5 [01:05<04:22, 65.72s/it]Validation Accuracy: 0.8225524475524476\n","# Train loss: 0.37277104894388213\n","# Epoch:  40%|████      | 2/5 [02:11<03:16, 65.67s/it]Validation Accuracy: 0.875\n","# Train loss: 0.2984921696880773\n","# Epoch:  60%|██████    | 3/5 [03:16<02:11, 65.63s/it]Validation Accuracy: 0.9155375874125874\n","# Train loss: 0.24781449089787866\n","# Epoch:  80%|████████  | 4/5 [04:22<01:05, 65.60s/it]Validation Accuracy: 0.9332386363636364\n","# Train loss: 0.2031506095761986\n","# Epoch: 100%|██████████| 5/5 [05:27<00:00, 65.58s/it]Validation Accuracy: 0.9340034965034966\n","\n","\n","# Zero Shot Model for test: English \n","\n","# Weighted F1 Score: 0.7568607824813645\n","# Macro F1 Score: 0.5590009465496957\n","# Accuracy score: 0.7945288638476278 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2018\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.36120438524302834\n","# Epoch:  20%|██        | 1/5 [11:36<46:24, 696.05s/it]Validation Accuracy: 0.8508093070308548\n","# Train loss: 0.31638229705365856\n","# Epoch:  40%|████      | 2/5 [23:12<34:48, 696.23s/it]Validation Accuracy: 0.8565882650480526\n","# Train loss: 0.2834163580750798\n","# Epoch:  60%|██████    | 3/5 [34:50<23:13, 696.61s/it]Validation Accuracy: 0.8612417804754678\n","# Train loss: 0.24601886915670876\n","# Epoch:  80%|████████  | 4/5 [46:27<11:36, 696.88s/it]Validation Accuracy: 0.8590477996965098\n","# Train loss: 0.2116280013976795\n","# Epoch: 100%|██████████| 5/5 [58:04<00:00, 696.87s/it]Validation Accuracy: 0.8616021750126455\n","\n","# Weighted F1 Score: 0.8611457140498046\n","# Macro F1 Score: 0.7736580630256132\n","# Accuracy score: 0.8633690600398368 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2019\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3881661957156082\n","# Epoch:  20%|██        | 1/5 [11:36<46:25, 696.37s/it]Validation Accuracy: 0.8485078401618613\n","# Train loss: 0.3237153235423172\n","# Epoch:  40%|████      | 2/5 [23:13<34:49, 696.64s/it]Validation Accuracy: 0.868063985837127\n","# Train loss: 0.2905816528316262\n","# Epoch:  60%|██████    | 3/5 [34:50<23:13, 696.65s/it]Validation Accuracy: 0.8712000505816894\n","# Train loss: 0.2585893683648312\n","# Epoch:  80%|████████  | 4/5 [46:26<11:36, 696.58s/it]Validation Accuracy: 0.8652250885179564\n","# Train loss: 0.2260776189386638\n","# Epoch: 100%|██████████| 5/5 [58:03<00:00, 696.62s/it]Validation Accuracy: 0.868715225088518\n","\n","# Weighted F1 Score: 0.8601001324082551\n","# Macro F1 Score: 0.7737363834608356\n","# Accuracy score: 0.8611875177843119 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2020\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.35840046078607896\n","# Epoch:  20%|██        | 1/5 [11:37<46:29, 697.38s/it]Validation Accuracy: 0.8592501264542235\n","# Train loss: 0.3137555369970411\n","# Epoch:  40%|████      | 2/5 [23:14<34:51, 697.26s/it]Validation Accuracy: 0.8637013151239251\n","# Train loss: 0.31825844710622103\n","# Epoch:  60%|██████    | 3/5 [34:50<23:13, 696.92s/it]Validation Accuracy: 0.8586747597369753\n","# Train loss: 0.2708698518359426\n","# Epoch:  80%|████████  | 4/5 [46:26<11:36, 696.80s/it]Validation Accuracy: 0.8665591805766313\n","# Train loss: 0.24352293666480435\n","# Epoch: 100%|██████████| 5/5 [58:03<00:00, 696.68s/it]Validation Accuracy: 0.8598191704602933\n","\n","# Weighted F1 Score: 0.8637769310424932\n","# Macro F1 Score: 0.7766964910627228\n","# Accuracy score: 0.8667836479180498 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2021\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.35709880783535347\n","# Epoch:  20%|██        | 1/5 [11:37<46:28, 697.12s/it]Validation Accuracy: 0.8568538189175517\n","# Train loss: 0.3194343811056502\n","# Epoch:  40%|████      | 2/5 [23:14<34:51, 697.27s/it]Validation Accuracy: 0.8671029337379869\n","# Train loss: 0.2848335175358894\n","# Epoch:  60%|██████    | 3/5 [34:51<23:14, 697.12s/it]Validation Accuracy: 0.8650227617602427\n","# Train loss: 0.24903473242715166\n","# Epoch:  80%|████████  | 4/5 [46:26<11:36, 696.53s/it]Validation Accuracy: 0.866831057157309\n","# Train loss: 0.21495501431375064\n","# Epoch: 100%|██████████| 5/5 [58:02<00:00, 696.47s/it]Validation Accuracy: 0.8689997470915529\n","\n","# Weighted F1 Score: 0.857066481971672\n","# Macro F1 Score: 0.7660983043582515\n","# Accuracy score: 0.8599544721616238 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2022\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.358400487027854\n","# Epoch:  20%|██        | 1/5 [11:35<46:22, 695.57s/it]Validation Accuracy: 0.8569613050075872\n","# Train loss: 0.312022206006677\n","# Epoch:  40%|████      | 2/5 [23:11<34:47, 695.69s/it]Validation Accuracy: 0.8667488619119879\n","# Train loss: 0.27882615593095456\n","# Epoch:  60%|██████    | 3/5 [34:48<23:12, 696.01s/it]Validation Accuracy: 0.8677794638340921\n","# Train loss: 0.23995585219238866\n","# Epoch:  80%|████████  | 4/5 [46:25<11:36, 696.22s/it]Validation Accuracy: 0.868259989883662\n","# Train loss: 0.203045981643415\n","# Epoch: 100%|██████████| 5/5 [58:01<00:00, 696.26s/it]Validation Accuracy: 0.8654906423874558\n","\n","# Weighted F1 Score: 0.8605045555914574\n","# Macro F1 Score: 0.7719418264175988\n","# Accuracy score: 0.8631793607132695 \n","\n","# ====================================================================================================\n","# The Average  Weighted F1-Score of the Language  English is: 0.8605187630127364\n","# The Average  Macro F1-Score of the Language  English is: 0.7724262136650044\n","# ========================================================================================================================================================================================================\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3685951924133457\n","# Epoch:  20%|██        | 1/5 [17:19<1:09:16, 1039.12s/it]Validation Accuracy: 0.8521012931034483\n","# Train loss: 0.319102366432524\n","# Epoch:  40%|████      | 2/5 [34:38<51:57, 1039.12s/it]  Validation Accuracy: 0.8750897988505747\n","# Train loss: 0.2926776656365672\n","# Epoch:  60%|██████    | 3/5 [51:57<34:38, 1039.19s/it]Validation Accuracy: 0.8901760057471264\n","# Train loss: 0.2657543064785499\n","# Epoch:  80%|████████  | 4/5 [1:09:17<17:19, 1039.26s/it]Validation Accuracy: 0.905621408045977\n","# Train loss: 0.23953153425084012\n","# Epoch: 100%|██████████| 5/5 [1:26:37<00:00, 1039.42s/it]Validation Accuracy: 0.9135237068965517\n","\n","\n","# Zero Shot Model for test: French \n","\n","# Weighted F1 Score: 0.6582832958912695\n","# Macro F1 Score: 0.584831632430229\n","# Accuracy score: 0.6934426229508197 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2018\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.5960467568150273\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.17s/it]Validation Accuracy: 0.653125\n","# Train loss: 0.49616401201045074\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.18s/it]Validation Accuracy: 0.6375\n","# Train loss: 0.4077246556127513\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.21s/it]Validation Accuracy: 0.6171875\n","# Train loss: 0.29622458921814404\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.21s/it]Validation Accuracy: 0.5890625\n","# Train loss: 0.22328302146935905\n","# Epoch: 100%|██████████| 5/5 [00:41<00:00,  8.21s/it]Validation Accuracy: 0.68125\n","\n","# Weighted F1 Score: 0.7303481697527759\n","# Macro F1 Score: 0.6950693024312656\n","# Accuracy score: 0.7295081967213115 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2019\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.5985802207831983\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.13s/it]Validation Accuracy: 0.640625\n","# Train loss: 0.48058044413725537\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]Validation Accuracy: 0.6328125\n","# Train loss: 0.35988784098514803\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]Validation Accuracy: 0.625\n","# Train loss: 0.25943213659856057\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.12s/it]Validation Accuracy: 0.6671875\n","# Train loss: 0.16485585310254935\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]Validation Accuracy: 0.6578125\n","\n","# Weighted F1 Score: 0.7266684429735377\n","# Macro F1 Score: 0.6869118905047049\n","# Accuracy score: 0.7295081967213115 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2020\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.6177335193863621\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]Validation Accuracy: 0.684375\n","# Train loss: 0.48487425567927184\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]Validation Accuracy: 0.65625\n","# Train loss: 0.3934462702384702\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]Validation Accuracy: 0.696875\n","# Train loss: 0.30564809452604363\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.12s/it]Validation Accuracy: 0.68125\n","# Train loss: 0.23027063657840094\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]Validation Accuracy: 0.6984375\n","\n","# Weighted F1 Score: 0.6744010088272384\n","# Macro F1 Score: 0.6246153846153846\n","# Accuracy score: 0.680327868852459 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2021\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.596904817002791\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.11s/it]Validation Accuracy: 0.6734375\n","# Train loss: 0.4992177271180683\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]Validation Accuracy: 0.70625\n","# Train loss: 0.3879742696881294\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]Validation Accuracy: 0.7046875\n","# Train loss: 0.31147828339426603\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.11s/it]Validation Accuracy: 0.7015625\n","# Train loss: 0.1961522346569432\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]Validation Accuracy: 0.7296875\n","\n","# Weighted F1 Score: 0.7568531887721858\n","# Macro F1 Score: 0.7187599364069952\n","# Accuracy score: 0.7622950819672131 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2022\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.5771397291510193\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.10s/it]Validation Accuracy: 0.6125\n","# Train loss: 0.4623527571007057\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.10s/it]Validation Accuracy: 0.6484375\n","# Train loss: 0.36395569504411135\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.10s/it]Validation Accuracy: 0.5765625\n","# Train loss: 0.20475372199521022\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]Validation Accuracy: 0.6140625\n","# Train loss: 0.12859782145393114\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]Validation Accuracy: 0.5984375\n","\n","# Weighted F1 Score: 0.6941114907723125\n","# Macro F1 Score: 0.660590087122044\n","# Accuracy score: 0.6885245901639344 \n","\n","# ====================================================================================================\n","# The Average  Weighted F1-Score of the Language  French is: 0.7164764602196101\n","# The Average  Macro F1-Score of the Language  French is: 0.6771893202160788\n","# ========================================================================================================================================================================================================\n","# Average Weighted F1-Score 0.8998807266942185 and Average Macro F1-Score 0.8482725556661921 of Arabic \n","\n","# Average Weighted F1-Score 0.8605187630127364 and Average Macro F1-Score 0.7724262136650044 of English \n","\n","# Average Weighted F1-Score 0.7164764602196101 and Average Macro F1-Score 0.6771893202160788 of French \n"],"execution_count":null,"outputs":[]}]}