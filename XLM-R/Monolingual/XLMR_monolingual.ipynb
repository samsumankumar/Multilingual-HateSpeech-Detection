{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"XLMR_monolingual.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"0sqFkr4OnnYQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606182482236,"user_tz":300,"elapsed":7533,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"6615f2fa-514c-4afb-cfa6-e850d1f45e57"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\r\u001b[K     |▎                               | 10kB 21.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.7MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 18.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 51kB 17.0MB/s eta 0:00:01\r\u001b[K     |█▌                              | 61kB 15.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 71kB 14.7MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 15.6MB/s eta 0:00:01\r\u001b[K     |██▎                             | 92kB 14.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 102kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▊                             | 112kB 13.5MB/s eta 0:00:01\r\u001b[K     |███                             | 122kB 13.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 133kB 13.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 143kB 13.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 153kB 13.5MB/s eta 0:00:01\r\u001b[K     |████                            | 163kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 174kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 184kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 194kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 204kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 215kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 225kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 235kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 245kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 256kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 266kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 276kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 286kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 296kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 307kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 317kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 327kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 337kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 348kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 358kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 368kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 378kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 389kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 399kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 409kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 419kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 430kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 440kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 450kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 460kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 471kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 481kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 491kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 501kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 512kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 522kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 532kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 542kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 552kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 563kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 573kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 583kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 593kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 604kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 614kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 624kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 634kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 645kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 655kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 665kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 675kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 686kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 696kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 706kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 716kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 727kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 737kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 747kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 757kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 768kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 778kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 788kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 798kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 808kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 819kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 829kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 839kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 849kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 860kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 870kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 880kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 890kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 901kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 911kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 921kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 931kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 942kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 952kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 962kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 972kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 983kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 993kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.3MB 13.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 60.2MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 63.5MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 49.0MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=e746800b9f006c12784e7c985d33894e53d5f6ca15525a4c6ff1af1a9684c770\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZH08Ma-oaYE","executionInfo":{"status":"ok","timestamp":1606182485907,"user_tz":300,"elapsed":10809,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"ca26b8bf-0fc4-40c1-9370-a88a930f9c65"},"source":[" !pip install emoji"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/1c/1f1457fe52d0b30cbeebfd578483cedb3e3619108d2d5a21380dfecf8ffd/emoji-0.6.0.tar.gz (51kB)\n","\r\u001b[K     |██████▍                         | 10kB 20.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 20kB 25.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 30kB 19.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.6.0-cp36-none-any.whl size=49716 sha256=68d5eeb1bda8e47e0d6f95dfca6fe6d061a3e482efad8a5a386c29b1de7a1aa0\n","  Stored in directory: /root/.cache/pip/wheels/46/2c/8b/9dcf5216ca68e14e0320e283692dce8ae321cdc01e73e17796\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.6.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7A8Wt1WgeMUz","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1606182491635,"user_tz":300,"elapsed":15002,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"b313e76d-edc6-4277-d0ff-54da3b8e758d"},"source":["import warnings\n","\n","warnings.simplefilter(\"ignore\", UserWarning)\n","warnings.simplefilter(\"ignore\", FutureWarning)\n","warnings.simplefilter(\"ignore\", DeprecationWarning)\n","\n","import tensorflow as tf\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import os\n","import numpy as np\n","from sklearn.metrics import f1_score, accuracy_score\n","from statistics import mode\n","\n","import re\n","import emoji\n","import random\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, TweetTokenizer\n","from nltk.corpus import wordnet, stopwords\n","from imblearn.over_sampling import SMOTE\n","\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla V100-SXM2-16GB'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"f9G65YtrnnYQ"},"source":["def preprocess(df):\n","    \n","    #removes URL\n","    pattern = r'https.?://[^\\s]+[\\s]?'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes usernames/mentions\n","    pattern = r'@[^\\s]+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes emoji and smiley\n","    pattern = re.compile(\"[\"\n","                         u\"\\U0001F600-\\U0001F64F\"\n","                         u\"\\U0001F300-\\U0001F5FF\"\n","                         u\"\\U0001F680-\\U0001F6FF\"\n","                         u\"\\U0001F1E0-\\U0001F1FF\"\n","                         u\"\\U00002500-\\U00002BEF\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U000024C2-\\U0001F251\"\n","                         u\"\\U0001f926-\\U0001f937\"\n","                         u\"\\U00010000-\\U0010ffff\"\n","                         u\"\\u2640-\\u2642\"\n","                         u\"\\u2600-\\u2B55\"\n","                         u\"\\u200d\"\n","                         u\"\\u23cf\"\n","                         u\"\\u23e9\"\n","                         u\"\\u231a\"\n","                         u\"\\ufe0f\"\n","                         u\"\\u3030\"\n","                         \"]+\", flags=re.UNICODE)\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes numbers\n","    pattern = r'\\d+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes punctuation\n","    pattern = r\"[^\\w\\s]\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","\n","    #removes stop words\n","    stop_words = stopwords.words(\"english\")    \n","    remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \")\n","                                              if token not in stop_words])\n","    df[\"tweet\"] = df[\"tweet\"].apply(remove_stop_words)\n","    \n","    #removes extra spaces\n","    pattern = r\"[\\s]+\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","    \n","    return(df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w264IlP9ghob"},"source":["def train_validate_test_split(df,seed, train_percent=.8, validate_percent=.125):\n","    train, test = train_test_split(df, train_size=train_percent, stratify=df['label'])\n","    train, validate = train_test_split(train, test_size=validate_percent, stratify=train['label'])\n","    return train, validate, test\n","\n","def tokenize_data(df, sampling=False):\n","    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in df['tweet']]\n","    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    \n","    MAX_LEN = 128\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    \n","    if(sampling==True):\n","      input_ids, labels = oversampling(input_ids, df.label)\n","    else:\n","      labels = df['label'].copy()\n","\n","    # Create attention masks\n","    attention_masks = []\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return input_ids, attention_masks, labels\n","\n","def Data_Loader(inputs_ids, attention_masks, labels, batch_size=16): \n","    data = TensorDataset(torch.LongTensor(inputs_ids), torch.LongTensor(attention_masks), torch.LongTensor(labels.ravel()))\n","    sampler = RandomSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n","    return dataloader\n","\n","def oversampling(ids, label):\n","  over = SMOTE(kind='svm', k_neighbors=2)\n","  sampled = over.fit_sample(ids, label)\n","  labels = sampled[1].copy()\n","  input_ids = sampled[0].copy()\n","  return input_ids, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lRd06Ne4ghq5"},"source":["def model_train(model, train_dataloader, validation_dataloader):\n","    epochs = 5\n","    for _ in trange(epochs, desc=\"Epoch\"):  \n","        model.train()\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        for step, batch in enumerate(train_dataloader):\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            optimizer.zero_grad()\n","            loss, _ = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","            loss.backward()\n","            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","            optimizer.step()\n","            scheduler.step()\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","\n","        ## VALIDATION\n","\n","        model.eval()\n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        for batch in validation_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            with torch.no_grad():\n","                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n","            eval_accuracy += tmp_eval_accuracy\n","            nb_eval_steps += 1\n","        validation_accuracy = (eval_accuracy/nb_eval_steps)\n","        print(\"Validation Accuracy: {}\".format(validation_accuracy))\n","    return validation_accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsnuBqImghv2"},"source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_test(model,prediction_dataloader):\n","    model.eval()\n","    predictions , true_labels = [], []\n","    for batch in prediction_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        predictions+=list(np.argmax(logits, axis=1).flatten())\n","        true_labels+=list(label_ids.flatten())\n","    weighted_f1_score = f1_score(true_labels, predictions, average= 'weighted')\n","    print(\"Weighted F1 Score:\",weighted_f1_score)\n","    macro_f1_score = f1_score(true_labels, predictions, average= 'macro')\n","    print(\"Macro F1 Score:\",macro_f1_score)    \n","    test_accuracy_score = accuracy_score(true_labels, predictions)\n","    print(\"Accuracy score:\", test_accuracy_score, \"\\n\")\n","    print(\"=\"*100)\n","    return weighted_f1_score, macro_f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLCuCukimkNk"},"source":["def model_initialise(dataloader_len):\n","    model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels = 2, output_attentions = False, output_hidden_states = False).cuda()\n","    optimizer = optim.AdamW(params = model.parameters(), lr=1e-5)\n","    epochs = 5\n","    total_steps = dataloader_len * epochs\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n","    return model, optimizer, scheduler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCiYd-UpO_q3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606203437698,"user_tz":300,"elapsed":19198646,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"765a4398-97e4-4ff4-e17c-add104ae4878"},"source":["print_stmts= []\n","languages = ['French','Arabic','English']\n","directory = './'\n","for lang in languages:\n","    df = pd.read_csv(os.path.join(directory, lang+'.csv'))\n","    df = preprocess(df)\n","    weighted = []\n","    macro = []\n","    seeds = [2018, 2019, 2020, 2021, 2022]\n","    for seed in seeds:\n","        np.random.seed(seed)\n","        train_df, validation_df, test_df = train_validate_test_split(df, seed)\n","        train_input_ids, train_attention_masks, train_labels = tokenize_data(train_df)\n","        train_dataloader = Data_Loader(train_input_ids, train_attention_masks, train_labels)\n","        validation_input_ids, validation_attention_masks, validation_labels = tokenize_data(validation_df)\n","        validation_dataloader = Data_Loader(validation_input_ids, validation_attention_masks, validation_labels)\n","        print(\"\\nModel Summary:\")\n","        print('Language:', lang)\n","        print('Seed value:', seed)\n","        model, optimizer, scheduler = model_initialise(len(train_dataloader))\n","        validation_accuracy = model_train(model, train_dataloader, validation_dataloader)\n","        test_input_ids, test_attention_masks, test_labels = tokenize_data(test_df)\n","        test_dataloader = Data_Loader(test_input_ids, test_attention_masks, test_labels)\n","        w, m = model_test(model, test_dataloader)\n","        weighted.append(w)\n","        macro.append(m)\n","    print(\"The Average  Weighted F1-Score of the Language \", lang, \"is:\",sum(weighted)/ len(weighted))\n","    print(\"The Average  Macro F1-Score of the Language \", lang, \"is:\",sum(macro)/ len(macro))\n","    print(\"=\"*200)\n","    print_stmts.append(\"Average Weighted F1-Score \"+str(sum(weighted)/len(weighted))+\" and Average Macro F1-Score \"+str(sum(macro)/len(macro))+\"of \"+ str(lang))\n","    for i in print_stmts:\n","        print(i,\"\\n\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Model Summary:\n","Language: French\n","Seed value: 2018\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.6648084995923219\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6640625\n","Train loss: 0.6358429474963082\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n","Train loss: 0.6332687608621739\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.66875\n","Train loss: 0.6331957964985458\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n","Train loss: 0.6321470649154098\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.5403407264545163\n","Macro F1 Score: 0.40196078431372545\n","Accuracy score: 0.6721311475409836 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2019\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.662671575391734\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.678125\n","Train loss: 0.64702868020093\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6640625\n","Train loss: 0.6367832019373223\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.678125\n","Train loss: 0.6208177716643722\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.678125\n","Train loss: 0.6147772862955376\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.678125\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.5403407264545163\n","Macro F1 Score: 0.40196078431372545\n","Accuracy score: 0.6721311475409836 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2020\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.6607458823257022\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6640625\n","Train loss: 0.6521421461193649\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6828125\n","Train loss: 0.6325672495144384\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6546875\n","Train loss: 0.6317089762952592\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.66875\n","Train loss: 0.636870437198215\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.66875\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.5403407264545163\n","Macro F1 Score: 0.40196078431372545\n","Accuracy score: 0.6721311475409836 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2021\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.6446397966808743\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n","Train loss: 0.6372946743611936\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.66875\n","Train loss: 0.6318548261015503\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6484375\n","Train loss: 0.5878503697889822\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.671875\n","Train loss: 0.5733188401769709\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.659375\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.6475945569484625\n","Macro F1 Score: 0.5721345448748605\n","Accuracy score: 0.6844262295081968 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: French\n","Seed value: 2022\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.647764918980775\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:08<00:32,  8.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6828125\n","Train loss: 0.6489278022889737\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6640625\n","Train loss: 0.6414708925618066\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n","Train loss: 0.6140303335807942\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [00:32<00:08,  8.12s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6734375\n","Train loss: 0.5890485224900422\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.6953125\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.5403407264545163\n","Macro F1 Score: 0.40196078431372545\n","Accuracy score: 0.6721311475409836 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  French is: 0.5617914925533054\n","The Average  Macro F1-Score of the Language  French is: 0.4359955364259525\n","========================================================================================================================================================================================================\n","Average Weighted F1-Score0.5617914925533054and Average Macro F1-Score0.4359955364259525of the languageFrench \n","\n","====================================================================================================French====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2018\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.49767999532889196\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8513513513513513\n","Train loss: 0.35226357879307796\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.34s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8682432432432432\n","Train loss: 0.31190237089052913\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.857545045045045\n","Train loss: 0.28807886981764647\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8643018018018018\n","Train loss: 0.2649318701990946\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.37s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8626126126126127\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8838678180771445\n","Macro F1 Score: 0.8193414965986395\n","Accuracy score: 0.8886010362694301 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2019\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.4823182058029287\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.856418918918919\n","Train loss: 0.325013680735559\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.40s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8868243243243243\n","Train loss: 0.2636119943537463\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.41s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.902027027027027\n","Train loss: 0.22077986609014705\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8868243243243243\n","Train loss: 0.18858723704681152\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8896396396396395\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8943275265286397\n","Macro F1 Score: 0.8411522633744857\n","Accuracy score: 0.8946459412780656 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2020\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.5269669220438153\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.39s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.7865990990990991\n","Train loss: 0.417481966404699\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.42s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.875\n","Train loss: 0.31447276191448603\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8817567567567568\n","Train loss: 0.26241794375218747\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8834459459459459\n","Train loss: 0.23469854246340985\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8783783783783784\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.9161721625618257\n","Macro F1 Score: 0.8742717452258387\n","Accuracy score: 0.9162348877374784 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2021\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.4744490548146991\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8733108108108109\n","Train loss: 0.3323861544467802\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8834459459459459\n","Train loss: 0.2704096885295365\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8778153153153152\n","Train loss: 0.23662704582084118\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8811936936936936\n","Train loss: 0.210832382383661\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8834459459459459\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.9017544599136803\n","Macro F1 Score: 0.8543881997255496\n","Accuracy score: 0.9006908462867013 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: Arabic\n","Seed value: 2022\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.4759269176036354\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [00:38<02:33, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8676801801801801\n","Train loss: 0.32127251455516326\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [01:16<01:55, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8868243243243243\n","Train loss: 0.2551351697294144\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [01:55<01:16, 38.43s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.893581081081081\n","Train loss: 0.21160431389557563\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [02:33<00:38, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8952702702702703\n","Train loss: 0.1770935259341431\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8918918918918919\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8982662576738617\n","Macro F1 Score: 0.8464982693865528\n","Accuracy score: 0.8989637305699482 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  Arabic is: 0.8988776449510304\n","The Average  Macro F1-Score of the Language  Arabic is: 0.8471303948622133\n","========================================================================================================================================================================================================\n","Average Weighted F1-Score0.5617914925533054and Average Macro F1-Score0.4359955364259525of the languageFrench \n","\n","Average Weighted F1-Score0.8988776449510304and Average Macro F1-Score0.8471303948622133of the languageArabic \n","\n","====================================================================================================Arabic====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2018\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3683812754354287\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:38<46:33, 698.49s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.859129994941831\n","Train loss: 0.3160639904215754\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:17<34:56, 698.69s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8669258978249874\n","Train loss: 0.28706888220848953\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:56<23:17, 698.86s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8666350531107738\n","Train loss: 0.26255775001885256\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:35<11:38, 698.70s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8645612038442083\n","Train loss: 0.23857382561691684\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:14<00:00, 698.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8637076378351036\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8647974263572064\n","Macro F1 Score: 0.7813540293511745\n","Accuracy score: 0.8658351512852129 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2019\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.36447684346511877\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:38<46:34, 698.69s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8676909458775922\n","Train loss: 0.31646106722277106\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:17<34:56, 698.75s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8686203844208397\n","Train loss: 0.2855927807737425\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:56<23:17, 698.85s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8704350025290845\n","Train loss: 0.26328138579042504\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:35<11:38, 698.83s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8663568538189176\n","Train loss: 0.2400031902544877\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:14<00:00, 698.97s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8675961052099139\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8588953965463227\n","Macro F1 Score: 0.7707407004993247\n","Accuracy score: 0.8606658446362515 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2020\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3671970392918344\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:38<46:34, 698.68s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.862373545776429\n","Train loss: 0.31486930572499483\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:17<34:55, 698.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8627529084471421\n","Train loss: 0.2879878534906544\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:55<23:17, 698.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.869663631765301\n","Train loss: 0.26159273587557313\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:33<11:38, 698.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8698659585230146\n","Train loss: 0.23927275580651824\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:13<00:00, 698.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8680703085483055\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8627163383793262\n","Macro F1 Score: 0.7756112313720112\n","Accuracy score: 0.8653134781371526 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2021\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.3658158818941015\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:39<46:36, 699.20s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.864270359129995\n","Train loss: 0.3129915121487293\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:16<34:56, 698.74s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8564871016691957\n","Train loss: 0.28509787482417653\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:54<23:16, 698.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8627529084471421\n","Train loss: 0.25811701204321685\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:33<11:38, 698.53s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.863321952453212\n","Train loss: 0.2355335181511966\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:12<00:00, 698.58s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8639858371269601\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.858353043827432\n","Macro F1 Score: 0.7701313790851088\n","Accuracy score: 0.8599544721616238 \n","\n","====================================================================================================\n","\n","Model Summary:\n","Language: English\n","Seed value: 2022\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Train loss: 0.36561783764542843\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  20%|██        | 1/5 [11:38<46:35, 698.86s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8469081942336874\n","Train loss: 0.31494393829137585\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  40%|████      | 2/5 [23:16<34:55, 698.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8618045017703592\n","Train loss: 0.2888010124965239\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  60%|██████    | 3/5 [34:54<23:16, 698.38s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8631133029843197\n","Train loss: 0.2635283211095522\n"],"name":"stdout"},{"output_type":"stream","text":["\rEpoch:  80%|████████  | 4/5 [46:33<11:38, 698.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8645422357106728\n","Train loss: 0.24019365908233611\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch: 100%|██████████| 5/5 [58:13<00:00, 698.70s/it]"],"name":"stderr"},{"output_type":"stream","text":["Validation Accuracy: 0.8651239251390996\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Weighted F1 Score: 0.8609575842541556\n","Macro F1 Score: 0.7716388924058272\n","Accuracy score: 0.8643175566726738 \n","\n","====================================================================================================\n","The Average  Weighted F1-Score of the Language  English is: 0.8611439578728886\n","The Average  Macro F1-Score of the Language  English is: 0.7738952465426893\n","========================================================================================================================================================================================================\n","Average Weighted F1-Score0.5617914925533054and Average Macro F1-Score0.4359955364259525of the languageFrench \n","\n","Average Weighted F1-Score0.8988776449510304and Average Macro F1-Score0.8471303948622133of the languageArabic \n","\n","Average Weighted F1-Score0.8611439578728886and Average Macro F1-Score0.7738952465426893of the languageEnglish \n","\n","====================================================================================================English====================================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mrOVEMXqy_aK"},"source":["# Model Summary:\n","# Language: French\n","# Seed value: 2018\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.6648084995923219\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]Validation Accuracy: 0.6640625\n","# Train loss: 0.6358429474963082\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.11s/it]Validation Accuracy: 0.6734375\n","# Train loss: 0.6332687608621739\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]Validation Accuracy: 0.66875\n","# Train loss: 0.6331957964985458\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]Validation Accuracy: 0.6734375\n","# Train loss: 0.6321470649154098\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]Validation Accuracy: 0.6734375\n","\n","# Weighted F1 Score: 0.5403407264545163\n","# Macro F1 Score: 0.40196078431372545\n","# Accuracy score: 0.6721311475409836 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2019\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.662671575391734\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]Validation Accuracy: 0.678125\n","# Train loss: 0.64702868020093\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]Validation Accuracy: 0.6640625\n","# Train loss: 0.6367832019373223\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]Validation Accuracy: 0.678125\n","# Train loss: 0.6208177716643722\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.11s/it]Validation Accuracy: 0.678125\n","# Train loss: 0.6147772862955376\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]Validation Accuracy: 0.678125\n","\n","# Weighted F1 Score: 0.5403407264545163\n","# Macro F1 Score: 0.40196078431372545\n","# Accuracy score: 0.6721311475409836 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2020\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.6607458823257022\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]Validation Accuracy: 0.6640625\n","# Train loss: 0.6521421461193649\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.11s/it]Validation Accuracy: 0.6828125\n","# Train loss: 0.6325672495144384\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]Validation Accuracy: 0.6546875\n","# Train loss: 0.6317089762952592\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]Validation Accuracy: 0.66875\n","# Train loss: 0.636870437198215\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]Validation Accuracy: 0.66875\n","\n","# Weighted F1 Score: 0.5403407264545163\n","# Macro F1 Score: 0.40196078431372545\n","# Accuracy score: 0.6721311475409836 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2021\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.6446397966808743\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.12s/it]Validation Accuracy: 0.6734375\n","# Train loss: 0.6372946743611936\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.11s/it]Validation Accuracy: 0.66875\n","# Train loss: 0.6318548261015503\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.11s/it]Validation Accuracy: 0.6484375\n","# Train loss: 0.5878503697889822\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.10s/it]Validation Accuracy: 0.671875\n","# Train loss: 0.5733188401769709\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.10s/it]Validation Accuracy: 0.659375\n","\n","# Weighted F1 Score: 0.6475945569484625\n","# Macro F1 Score: 0.5721345448748605\n","# Accuracy score: 0.6844262295081968 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: French\n","# Seed value: 2022\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.647764918980775\n","# Epoch:  20%|██        | 1/5 [00:08<00:32,  8.13s/it]Validation Accuracy: 0.6828125\n","# Train loss: 0.6489278022889737\n","# Epoch:  40%|████      | 2/5 [00:16<00:24,  8.12s/it]Validation Accuracy: 0.6640625\n","# Train loss: 0.6414708925618066\n","# Epoch:  60%|██████    | 3/5 [00:24<00:16,  8.12s/it]Validation Accuracy: 0.6734375\n","# Train loss: 0.6140303335807942\n","# Epoch:  80%|████████  | 4/5 [00:32<00:08,  8.12s/it]Validation Accuracy: 0.6734375\n","# Train loss: 0.5890485224900422\n","# Epoch: 100%|██████████| 5/5 [00:40<00:00,  8.11s/it]Validation Accuracy: 0.6953125\n","\n","# Weighted F1 Score: 0.5403407264545163\n","# Macro F1 Score: 0.40196078431372545\n","# Accuracy score: 0.6721311475409836 \n","\n","# ====================================================================================================\n","# The Average  Weighted F1-Score of the Language  French is: 0.5617914925533054\n","# The Average  Macro F1-Score of the Language  French is: 0.4359955364259525\n","# ========================================================================================================================================================================================================\n","# Average Weighted F1-Score0.5617914925533054and Average Macro F1-Score0.4359955364259525of the languageFrench \n","\n","# ====================================================================================================French====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2018\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.49767999532889196\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.35s/it]Validation Accuracy: 0.8513513513513513\n","# Train loss: 0.35226357879307796\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.34s/it]Validation Accuracy: 0.8682432432432432\n","# Train loss: 0.31190237089052913\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.35s/it]Validation Accuracy: 0.857545045045045\n","# Train loss: 0.28807886981764647\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.36s/it]Validation Accuracy: 0.8643018018018018\n","# Train loss: 0.2649318701990946\n","# Epoch: 100%|██████████| 5/5 [03:11<00:00, 38.37s/it]Validation Accuracy: 0.8626126126126127\n","\n","# Weighted F1 Score: 0.8838678180771445\n","# Macro F1 Score: 0.8193414965986395\n","# Accuracy score: 0.8886010362694301 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2019\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.4823182058029287\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.39s/it]Validation Accuracy: 0.856418918918919\n","# Train loss: 0.325013680735559\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.40s/it]Validation Accuracy: 0.8868243243243243\n","# Train loss: 0.2636119943537463\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.41s/it]Validation Accuracy: 0.902027027027027\n","# Train loss: 0.22077986609014705\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.43s/it]Validation Accuracy: 0.8868243243243243\n","# Train loss: 0.18858723704681152\n","# Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.43s/it]Validation Accuracy: 0.8896396396396395\n","\n","# Weighted F1 Score: 0.8943275265286397\n","# Macro F1 Score: 0.8411522633744857\n","# Accuracy score: 0.8946459412780656 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2020\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.5269669220438153\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.39s/it]Validation Accuracy: 0.7865990990990991\n","# Train loss: 0.417481966404699\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.42s/it]Validation Accuracy: 0.875\n","# Train loss: 0.31447276191448603\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.43s/it]Validation Accuracy: 0.8817567567567568\n","# Train loss: 0.26241794375218747\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.43s/it]Validation Accuracy: 0.8834459459459459\n","# Train loss: 0.23469854246340985\n","# Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]Validation Accuracy: 0.8783783783783784\n","\n","# Weighted F1 Score: 0.9161721625618257\n","# Macro F1 Score: 0.8742717452258387\n","# Accuracy score: 0.9162348877374784 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2021\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.4744490548146991\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.44s/it]Validation Accuracy: 0.8733108108108109\n","# Train loss: 0.3323861544467802\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.44s/it]Validation Accuracy: 0.8834459459459459\n","# Train loss: 0.2704096885295365\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.44s/it]Validation Accuracy: 0.8778153153153152\n","# Train loss: 0.23662704582084118\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.44s/it]Validation Accuracy: 0.8811936936936936\n","# Train loss: 0.210832382383661\n","# Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]Validation Accuracy: 0.8834459459459459\n","\n","# Weighted F1 Score: 0.9017544599136803\n","# Macro F1 Score: 0.8543881997255496\n","# Accuracy score: 0.9006908462867013 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: Arabic\n","# Seed value: 2022\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.4759269176036354\n","# Epoch:  20%|██        | 1/5 [00:38<02:33, 38.43s/it]Validation Accuracy: 0.8676801801801801\n","# Train loss: 0.32127251455516326\n","# Epoch:  40%|████      | 2/5 [01:16<01:55, 38.43s/it]Validation Accuracy: 0.8868243243243243\n","# Train loss: 0.2551351697294144\n","# Epoch:  60%|██████    | 3/5 [01:55<01:16, 38.43s/it]Validation Accuracy: 0.893581081081081\n","# Train loss: 0.21160431389557563\n","# Epoch:  80%|████████  | 4/5 [02:33<00:38, 38.44s/it]Validation Accuracy: 0.8952702702702703\n","# Train loss: 0.1770935259341431\n","# Epoch: 100%|██████████| 5/5 [03:12<00:00, 38.44s/it]Validation Accuracy: 0.8918918918918919\n","\n","# Weighted F1 Score: 0.8982662576738617\n","# Macro F1 Score: 0.8464982693865528\n","# Accuracy score: 0.8989637305699482 \n","\n","# ====================================================================================================\n","# The Average  Weighted F1-Score of the Language  Arabic is: 0.8988776449510304\n","# The Average  Macro F1-Score of the Language  Arabic is: 0.8471303948622133\n","# ========================================================================================================================================================================================================\n","# Average Weighted F1-Score0.5617914925533054and Average Macro F1-Score0.4359955364259525of the languageFrench \n","\n","# Average Weighted F1-Score0.8988776449510304and Average Macro F1-Score0.8471303948622133of the languageArabic \n","\n","# ====================================================================================================Arabic====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2018\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3683812754354287\n","# Epoch:  20%|██        | 1/5 [11:38<46:33, 698.49s/it]Validation Accuracy: 0.859129994941831\n","# Train loss: 0.3160639904215754\n","# Epoch:  40%|████      | 2/5 [23:17<34:56, 698.69s/it]Validation Accuracy: 0.8669258978249874\n","# Train loss: 0.28706888220848953\n","# Epoch:  60%|██████    | 3/5 [34:56<23:17, 698.86s/it]Validation Accuracy: 0.8666350531107738\n","# Train loss: 0.26255775001885256\n","# Epoch:  80%|████████  | 4/5 [46:35<11:38, 698.70s/it]Validation Accuracy: 0.8645612038442083\n","# Train loss: 0.23857382561691684\n","# Epoch: 100%|██████████| 5/5 [58:14<00:00, 698.85s/it]Validation Accuracy: 0.8637076378351036\n","\n","# Weighted F1 Score: 0.8647974263572064\n","# Macro F1 Score: 0.7813540293511745\n","# Accuracy score: 0.8658351512852129 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2019\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.36447684346511877\n","# Epoch:  20%|██        | 1/5 [11:38<46:34, 698.69s/it]Validation Accuracy: 0.8676909458775922\n","# Train loss: 0.31646106722277106\n","# Epoch:  40%|████      | 2/5 [23:17<34:56, 698.75s/it]Validation Accuracy: 0.8686203844208397\n","# Train loss: 0.2855927807737425\n","# Epoch:  60%|██████    | 3/5 [34:56<23:17, 698.85s/it]Validation Accuracy: 0.8704350025290845\n","# Train loss: 0.26328138579042504\n","# Epoch:  80%|████████  | 4/5 [46:35<11:38, 698.83s/it]Validation Accuracy: 0.8663568538189176\n","# Train loss: 0.2400031902544877\n","# Epoch: 100%|██████████| 5/5 [58:14<00:00, 698.97s/it]Validation Accuracy: 0.8675961052099139\n","\n","# Weighted F1 Score: 0.8588953965463227\n","# Macro F1 Score: 0.7707407004993247\n","# Accuracy score: 0.8606658446362515 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2020\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3671970392918344\n","# Epoch:  20%|██        | 1/5 [11:38<46:34, 698.68s/it]Validation Accuracy: 0.862373545776429\n","# Train loss: 0.31486930572499483\n","# Epoch:  40%|████      | 2/5 [23:17<34:55, 698.58s/it]Validation Accuracy: 0.8627529084471421\n","# Train loss: 0.2879878534906544\n","# Epoch:  60%|██████    | 3/5 [34:55<23:17, 698.53s/it]Validation Accuracy: 0.869663631765301\n","# Train loss: 0.26159273587557313\n","# Epoch:  80%|████████  | 4/5 [46:33<11:38, 698.54s/it]Validation Accuracy: 0.8698659585230146\n","# Train loss: 0.23927275580651824\n","# Epoch: 100%|██████████| 5/5 [58:13<00:00, 698.61s/it]Validation Accuracy: 0.8680703085483055\n","\n","# Weighted F1 Score: 0.8627163383793262\n","# Macro F1 Score: 0.7756112313720112\n","# Accuracy score: 0.8653134781371526 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2021\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.3658158818941015\n","# Epoch:  20%|██        | 1/5 [11:39<46:36, 699.20s/it]Validation Accuracy: 0.864270359129995\n","# Train loss: 0.3129915121487293\n","# Epoch:  40%|████      | 2/5 [23:16<34:56, 698.74s/it]Validation Accuracy: 0.8564871016691957\n","# Train loss: 0.28509787482417653\n","# Epoch:  60%|██████    | 3/5 [34:54<23:16, 698.48s/it]Validation Accuracy: 0.8627529084471421\n","# Train loss: 0.25811701204321685\n","# Epoch:  80%|████████  | 4/5 [46:33<11:38, 698.53s/it]Validation Accuracy: 0.863321952453212\n","# Train loss: 0.2355335181511966\n","# Epoch: 100%|██████████| 5/5 [58:12<00:00, 698.58s/it]Validation Accuracy: 0.8639858371269601\n","\n","# Weighted F1 Score: 0.858353043827432\n","# Macro F1 Score: 0.7701313790851088\n","# Accuracy score: 0.8599544721616238 \n","\n","# ====================================================================================================\n","\n","# Model Summary:\n","# Language: English\n","# Seed value: 2022\n","# Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","# - This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","# - This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","# Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","# You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","# Epoch:   0%|          | 0/5 [00:00<?, ?it/s]Train loss: 0.36561783764542843\n","# Epoch:  20%|██        | 1/5 [11:38<46:35, 698.86s/it]Validation Accuracy: 0.8469081942336874\n","# Train loss: 0.31494393829137585\n","# Epoch:  40%|████      | 2/5 [23:16<34:55, 698.48s/it]Validation Accuracy: 0.8618045017703592\n","# Train loss: 0.2888010124965239\n","# Epoch:  60%|██████    | 3/5 [34:54<23:16, 698.38s/it]Validation Accuracy: 0.8631133029843197\n","# Train loss: 0.2635283211095522\n","# Epoch:  80%|████████  | 4/5 [46:33<11:38, 698.62s/it]Validation Accuracy: 0.8645422357106728\n","# Train loss: 0.24019365908233611\n","# Epoch: 100%|██████████| 5/5 [58:13<00:00, 698.70s/it]Validation Accuracy: 0.8651239251390996\n","\n","# Weighted F1 Score: 0.8609575842541556\n","# Macro F1 Score: 0.7716388924058272\n","# Accuracy score: 0.8643175566726738 \n","\n","# ====================================================================================================\n","# The Average  Weighted F1-Score of the Language  English is: 0.8611439578728886\n","# The Average  Macro F1-Score of the Language  English is: 0.7738952465426893\n","# ========================================================================================================================================================================================================\n","# Average Weighted F1-Score0.5617914925533054and Average Macro F1-Score0.4359955364259525of the languageFrench \n","\n","# Average Weighted F1-Score0.8988776449510304and Average Macro F1-Score0.8471303948622133of the languageArabic \n","\n","# Average Weighted F1-Score0.8611439578728886and Average Macro F1-Score0.7738952465426893of the languageEnglish \n","\n","# ====================================================================================================English===================================================================================================="],"execution_count":null,"outputs":[]}]}