{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Error analysis_XLMR.ipynb","provenance":[{"file_id":"1u4iJUTzdsWUdQNBfVdgSX80Qi5QCj8-F","timestamp":1607045103558}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAq_Kdx9Y6Sf","executionInfo":{"status":"ok","timestamp":1607047344279,"user_tz":300,"elapsed":22873,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"3ec1c582-76bf-4e02-d5d2-dd6b49b7add1"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0sqFkr4OnnYQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607047755283,"user_tz":300,"elapsed":8095,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"e265c8fa-23a9-4b5d-e693-2cd643e8c9fe"},"source":["!pip install transformers\n","!pip install sentencepiece\n","!pip install emoji"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.0.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 13.6MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.94\n","Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7A8Wt1WgeMUz","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1607047766178,"user_tz":300,"elapsed":3250,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"97267a52-1b19-4f79-8ec4-af5e8dda8b2a"},"source":["import warnings\n","\n","warnings.simplefilter(\"ignore\", UserWarning)\n","warnings.simplefilter(\"ignore\", FutureWarning)\n","warnings.simplefilter(\"ignore\", DeprecationWarning)\n","\n","import tensorflow as tf\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import os\n","import numpy as np\n","from sklearn.metrics import f1_score, accuracy_score\n","from statistics import mode\n","\n","import re\n","import emoji\n","import random\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk import word_tokenize, pos_tag\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import sent_tokenize, TweetTokenizer\n","from nltk.corpus import wordnet, stopwords\n","\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla V100-SXM2-16GB'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"f9G65YtrnnYQ","executionInfo":{"status":"ok","timestamp":1607047772212,"user_tz":300,"elapsed":870,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def preprocess(df):\n","    \n","    #removes URL\n","    pattern = r'https.?://[^\\s]+[\\s]?'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes usernames/mentions\n","    pattern = r'@[^\\s]+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes emoji and smiley\n","    pattern = re.compile(\"[\"\n","                         u\"\\U0001F600-\\U0001F64F\"\n","                         u\"\\U0001F300-\\U0001F5FF\"\n","                         u\"\\U0001F680-\\U0001F6FF\"\n","                         u\"\\U0001F1E0-\\U0001F1FF\"\n","                         u\"\\U00002500-\\U00002BEF\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U00002702-\\U000027B0\"\n","                         u\"\\U000024C2-\\U0001F251\"\n","                         u\"\\U0001f926-\\U0001f937\"\n","                         u\"\\U00010000-\\U0010ffff\"\n","                         u\"\\u2640-\\u2642\"\n","                         u\"\\u2600-\\u2B55\"\n","                         u\"\\u200d\"\n","                         u\"\\u23cf\"\n","                         u\"\\u23e9\"\n","                         u\"\\u231a\"\n","                         u\"\\ufe0f\"\n","                         u\"\\u3030\"\n","                         \"]+\", flags=re.UNICODE)\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes numbers\n","    pattern = r'\\d+'\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\"\", regex=True)\n","    \n","    #removes punctuation\n","    pattern = r\"[^\\w\\s]\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","\n","    #removes stop words\n","    stop_words = stopwords.words(\"english\")    \n","    remove_stop_words = lambda row: \" \".join([token for token in row.split(\" \")\n","                                              if token not in stop_words])\n","    df[\"tweet\"] = df[\"tweet\"].apply(remove_stop_words)\n","    \n","    #removes extra spaces\n","    pattern = r\"[\\s]+\"\n","    df[\"tweet\"] = df[\"tweet\"].str.replace(pat=pattern, repl=\" \", regex=True)\n","    \n","    return(df)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"w264IlP9ghob","executionInfo":{"status":"ok","timestamp":1607047788469,"user_tz":300,"elapsed":906,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def tokenize_data(df, sampling=False):\n","    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in df['tweet']]\n","    tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    \n","    MAX_LEN = 100\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    \n","    labels = df['label'].copy()\n","\n","    # Create attention masks\n","    attention_masks = []\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return input_ids, attention_masks, labels\n","\n","def Data_Loader(inputs_ids, attention_masks, labels, batch_size=3): \n","    data = TensorDataset(torch.LongTensor(inputs_ids), torch.LongTensor(attention_masks), torch.LongTensor(labels.ravel()))\n","    sampler = RandomSampler(data)\n","    dataloader = DataLoader(data, sampler=sampler, batch_size=batch_size)\n","    return dataloader"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"OsnuBqImghv2","executionInfo":{"status":"ok","timestamp":1607047802606,"user_tz":300,"elapsed":718,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def model_test(model,prediction_dataloader):\n","    model.eval()\n","    predictions , true_labels = [], []\n","    for batch in prediction_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        with torch.no_grad():\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        predictions+=list(np.argmax(logits, axis=1).flatten())\n","        # true_labels+=list(label_ids.flatten())\n","    return predictions"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLCuCukimkNk","executionInfo":{"status":"ok","timestamp":1607047807374,"user_tz":300,"elapsed":521,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}}},"source":["def model_initialise(path= None , use_saved_model=False):\n","  model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels = 2, output_attentions = False, output_hidden_states = False).cuda()\n","\n","  if(use_saved_model==True):\n","    checkpoint = torch.load(path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","  return model"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"244HfUYQWnzS","executionInfo":{"status":"ok","timestamp":1607048407307,"user_tz":300,"elapsed":674,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"ce770121-efc4-4af0-8fc1-558573b6bea5"},"source":["from io import StringIO \n","test_string = \"\"\"tweet,label\n","                 Click below for a FREE download of a colorfully illustrated 132 page e-book on the Zionist-engineered INTENTIONAL destruction of Western civilization, 1\n","                 In my opinion using cuss words like bitch is demeaning to women, 0\n","                 I hope they all starve, 1\"\"\"\n","\n","data = io.StringIO(test_string)\n","df = pd.read_csv(data, sep=\",\")\n","print(df.tweet[0])\n","test_df = preprocess(df)\n","print(test_df.tweet[0])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["                 Click below for a FREE download of a colorfully illustrated 132 page e-book on the Zionist-engineered INTENTIONAL destruction of Western civilization\n"," Click FREE download colorfully illustrated page e book Zionist engineered INTENTIONAL destruction Western civilization\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pCiYd-UpO_q3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607048423246,"user_tz":300,"elapsed":14555,"user":{"displayName":"suman kumar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GicqVxGKhLDa6B_sPiIJNdXw6DBlngT2WcJJDwkwg=s64","userId":"06576966373551741897"}},"outputId":"3759bbb6-3c1d-4cfa-db53-62999baa3314"},"source":["model = model_initialise('/content/drive/My Drive/XLMRMultilingual models/XLMREnglish.pth',use_saved_model=True)\n","test_input_ids, test_attention_masks, test_labels = tokenize_data(test_df)\n","test_dataloader = Data_Loader(test_input_ids, test_attention_masks, test_labels)\n","pred = model_test(model, test_dataloader)\n","print(test_df, pred)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["                                               tweet  label\n","0   Click FREE download colorfully illustrated pa...      1\n","1   In opinion using cuss words like bitch demean...      0\n","2                                      I hope starve      1 [0, 0, 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2OUmco5biAyq"},"source":[""],"execution_count":null,"outputs":[]}]}